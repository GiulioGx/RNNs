The entirety of our work is based on the SGD framework described in section \ref{sec:sgd}. In particular we insisted on three phases of the algorithm, namely the initialization, the choice of the descent direction and the step size. We show that the initialization play a crucial role in the learning process and can, alone, dictate if the learning process will be "successful"\footnote{Here we refer to artificial task where a criterion for success is easily defined.} or not. We than propose a strategy to choose a descent direction which helps to deal with the vanishing gradient. As for the learning rate we use a technique which is entirely equivalent to the gradient clipping trick proposed in \cite{understandingExplodingGradients} which helps dealing with exploding gradients.


\section{Notation}
Before focusing on each phase we will introduce some notation which we will need in the following sections.

Consider a loss function $g$ for some time step $\tau$. Defining 
\begin{equation}
\nabla_{\mat{W_{rec}}} g_{|k}  \defeq \frac{\partial g}{\partial \vec{a}^{\tau}} \cdot \frac{\partial \vec{a}^{\tau}}{\partial \vec{a}^k} \cdot \frac{\partial^+ \vec{a}^k}{\partial \mat{W}^{rec}},
\end{equation}
and recalling the results of section \ref{sec:rnn_grad} we have
\begin{equation}
	\frac{\partial g_{\tau}}{\partial \mat{W}^{rec}} = \sum_{k=1}^{\tau} \nabla_{\mat{W_{rec}}} g_{|k}.
\end{equation}
We refer to $\nabla_{\vec{x}} g_{|k}$ as temporal gradient for time step $k$ w.r.t. the variable $\vec{x}$ and is easy to see that it is the gradient computed imagining to replicate variable $\vec{x}$ for each time step and taking the derivatives w.r.t. the variable for the step $k$.

The \textbf{vanishing gradient} problem appears then, under this new notation, when the norm of the temporal components $\nabla_{\vec{x}} g_{|k}$ are exponentially bigger for more recent time steps.

\section{Initialization}
The first moment of the learning process is the initialization of the variables. We found that the choice of the initial value for the recurrent matrix $\mat{W_{rec}}$ has a big impact on the entire learning process. Recall the results from section  \ref{sec:vanishing} where we saw that having such matrix too small singular values, more precisely $\sigma'_{max} \cdot \mu_{max} <1 $, is a sufficient condition for the gradient to vanish. Although a sufficient condition that, conversely, guarantees the gradient not to vanish is not known, the bounds on the singular values encourage to explore initialization techniques which produce matrices with higher singular values or more in general spectral radius. A similar suggestion, motivated by other consideration was given in the ESN field \cite{reservoirSummary}.

We propose an initialization scheme where the recurrent matrix is firstly sampled from some distribution\footnote{In all the experiments we always sample from a zero mean gaussian, but others distribution can be used as well.} and scaled to have a specified spectral radius as shown in \ref{algo:init_scaling}.

\begin{algorithm}[!h]
	\KwData{\\
		\Indp
		$\rho = $ desired spectral radius
	}
	\BlankLine

	$\mat{W_{rec}} \sim \mathcal{N}(0, \sigma^2)$\\
	$r \gets \mbox{spectral\_radius}(\mat{W_{rec}})$\\
	$\mat{W_{rec}}\gets \frac{\rho}{r} \cdot \mat{W_{rec}}$\\
	\KwRet{$\mat{W_{rec}}$}
	\caption{Recurrent weight matrix initialization scheme}
	\label{algo:init_scaling}
\end{algorithm}

 In figure \ref{fig:temporal_norms} we show, as an example, the temporal components w.r.t. all the variables of the model varying the spectral radius in [0.8, 0.9, 1, 1.1, 1.2] computed on a hundred samples for the temporal order task (INTRODUCE TASK).

\begin{figure}
    \includegraphics[width=0.9\textwidth]{chapter3/temporal_components.eps}
    \caption{Temporal components for the temporal order task varying the spectral radius of the recurrent matrix. y axis is in logarithmic scale.}
    \label{fig:temporal_norms}
\end{figure}

The first two cases, the one with spectral radius less than one, are perfect example of vanishing gradient instances: more recent temporal components have norm exponentially bigger than the more distant ones (please note that the y axis is in logarithmic scale). On the contrary such phenomenon is not observed in the cases of spectral radius bigger than one where the temporal components have roughly the same norm.

We found that, at least in the task we explored (SAY WHICH ONES), an appropriate spectral radius always allow the training process to start in a regime where the gradient does not vanish. We report some results on the effect of the initialization on the training process in chapter \ref{ch:experiments}

\section{Descent direction}

\begin{figure}
	\includegraphics[width=1\textwidth]{chapter3/compare_add_temp_norms_0.eps}
	\caption{Comparison between the temporal order task (first column) and the addition task (second column). First row shows the norms of the temporal components, while he second the cosine between each temporal component and the gradient. This is a snopshot taken at the first iteration of the training process.}
	\label{fig:comparison_add_temp_0}
\end{figure}

\begin{figure}
	\includegraphics[width=1\textwidth]{chapter3/compare_add_temp_norms_1.eps}
	\caption{Like in figure \ref{fig:comparison_add_temp_0}. This is a snapshot taken after a few iterations of the training process.}
	\label{fig:comparison_add_temp_1}
\end{figure}


The idea is to use the structure of the gradient to compute a "descent" direction which does not suffer from the vanishing problem.

\begin{itemize}
	\item normalize the temporal components:
	\begin{equation}
	g_t(\vec{x}) = \frac{\nabla L_{|t}(\vec{x})}{\norm{\nabla L_{|t}(\vec{x})}}.
	\end{equation}
	
	\item combine the normalized gradients in a simplex:
	\begin{equation}
	g(\vec{x}) = \sum_{t=1}^T \beta_t \cdot g_t(\vec{x}).
	\end{equation}
	
	with $\sum_{t=1}^T\beta_t=1, \beta_t>0$ (randomly picked at each iteration).
	\item exploit the gradient norm:
	\begin{equation}
	d(\vec{x}) = \norm{\nabla L (\vec{x})}\frac{g(\vec{x})}{\norm{g(\vec{x})}}.
	\end{equation}
\end{itemize}
\section{Learning rate}

The choice of the learning rate is crucial for the success of the training procedure. It is well known that RNNs give rise to gradients which change extremely fast in norm (the exploding gradient problem). This makes choosing a single constant step, or even designing an adaptive strategy, very difficult, at the least, for this kind of models. A simple trick which allow to choose a fixed step from the beginning is to \textit{clip the gradients}. This technique was introduced and used in slightly different forms in \cite{understandingExplodingGradients} and \cite{clippingMikolov} and we describe it in section \ref{algo:sgd}. We reformulate the version proposed in \cite{understandingExplodingGradients} as a learning rate selection strategy.
Given a direction $\vec{d}_k$ the step $\alpha_k$ is chosen as:
\begin{equation}
\alpha_k = 
\begin{cases}
	\mu  \quad &\mbox{if} \norm{\vec{d}_k}_2 \leq \tau\\
	\frac{\mu \cdot \tau}{\norm{\vec{d}_k}_2} \quad & otherwise,
\end{cases}
\end{equation}
where $\mu$ and $c$ are some positive constants. The parameter $\tau$ is the threshold on the direction norm; $\mu$ is instead the constant learning rate that is used when the norm of the direction is not above such threshold. The idea is to use a constant step when the direction has an enough small norm and vice-versa choose a step which inversely proportional when such norm is too big. We confirm, as found in works cited above, that this trick is essential to train RNNs in a stochastic framework.

\section{Putting all together}
Now that we have described all the three phases of the algorithm we can put them together, as done in algorithm \ref{algo:complete_solution} with a few additions. The main one them being a condition to switch the strategy used to compute the descent direction. The idea is to use the simplex combination at the beginning of the training process, or whenever the gradient is vanishing and switch back to the anti-gradient when appropriate. This is done by checking the norm of the gradient, as in line \ref{algo:line:condition}.


\begin{algorithm}[]
	\KwData{\\
		\Indp
		$D=\{\pair{\vec{x}^{(i)}}{\vec{y}^{(i)}}\}$: training set\\
		$m$: size of each mini-batch\\
		$\mu$: constant learning rate\\
		$\tau$: gradient clipping threshold \\
		$\rho$: initial spectral radius \\
		$\psi$ threshold for the direction norm
	}
	
	\KwResult{\\
		\Indp $\theta$: solution
	}
	\BlankLine
	
	$\mat{W_{rec}}, \mat{W_{in}, \mat{W_{out}}} \sim \mathcal{N}(0, \sigma^2)$\\
	$\vec{b}_{out}, \vec{b}_{rec} \gets 0$\\
	$r \gets \mbox{spectral\_radius}(\mat{W_{rec}})$\\
	$\mat{W_{rec}}\gets \frac{\rho}{r} \cdot \mat{W_{rec}}$\\
	$\theta_0 = [\mat{W_{rec}}, \mat{W_{in}}, \mat{W_{out}},\vec{b}_{out}, \vec{b}_{rec}]$

	
	\BlankLine
	\While{stop criterion}{
		
		$I$ $\gets$ sample $m$ training example $\in D$  \\
		$\{\nabla_\theta L_{|t}\} \gets \mbox{compute\_temporal\_gradients}(\theta_k, I)$\\
		$\vec{d}_k \gets \mbox{simplex\_combination}(\{\nabla_\theta L_{|t}\})$\\
		$\alpha \gets$ compute learning rate \\
		
		\If{$\norm{\nabla_{\theta_k}}_2 > \psi$}
			{$\vec{d}_k \gets \nabla_{\theta_k}$ \\
			\label{algo:line:condition}
		}
		
		$\alpha_k = 
		\begin{cases}
			\mu  \quad &\mbox{if} \norm{\vec{d}_k}_2 \leq \tau\\
			\frac{\mu \cdot \tau}{\norm{\vec{d}_k}_2} \quad & \mbox{otherwise}
		\end{cases}$\\
		
		$\theta_{k+1} \gets \theta_k - \alpha_k \vec{d}_k$\\
		$k\gets k+1$
	}
	\KwRet{$\theta_k$}
	\caption{RNN training}
	\label{algo:complete_solution}
\end{algorithm}

\section{Proof of convergence}
\input{chapter3/convergence}
