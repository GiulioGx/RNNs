 
In this thesis we introduce the Recurrent Neural Networks (RNN) model starting from the more common Feed Forward Neural Networks explaining how to use such models to define optimization problems for machine learning. We pay particular attention to the gradient structure of such models because it helps explaining the ``infamous'' \textit{exploding/vanishing} gradient problem which constitutes the most important difficulty in training RNNs. We explain how this problem affects the learning process and what solutions have been developed over time. We then introduce a novel approach based on stochastic gradient descent which deals with the \textit{exploding/vanishing} gradient problem with an appropriate initialization scheme and descent direction. Finally we evaluate our approach both on some synthetic task and real datasets.