\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{proof}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{pgf}
\usepackage{tikz}
\usepackage[noend]{algpseudocode}
\usepackage[ruled,noline,linesnumbered]{algorithm2e}
\usetikzlibrary{automata,arrows}
\let\emptyset\varnothing

\graphicspath{ {./images/} }

\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}

\title{On sliding gradient}
\author{Giulio Galvan}

\begin{document}
	\maketitle

\paragraph{Step bound for generic descent directions.}	
From Lipschitz continuity we get:
\begin{equation}
	f(y) \leq f(x) + \nabla f(x)^T(y-x) + \frac{L}{2}\norm{x-y}^2,
\end{equation}
which choosing $x=x_k$ and $y=x_{k+1}=x_k + t_k d_k$ becomes:
\begin{equation}
	f(x_{k+1})-f(x_k) \leq t_k\nabla f(x_k)^Td_k+ t_k^2\frac{L}{2}\norm{d_k}^2.
\end{equation}
The previous inequality can be rewritten as:
\begin{equation}
f(x_k)- f(x_{k+1}) \geq - t_k\nabla f(x_k)^Td_k\cdot \left(1+t_k\frac{L}{2}\frac{\nabla f(x_k)^Td_k}{\norm{\nabla f(x_k)}^2 cos^2\theta_k}\right)
\label{eq:lipschitzReworked}
\end{equation}
where
\begin{equation}
cos\theta_k=\frac{\nabla f(x_k)^Td_k}{\norm{\nabla f(x_k)}\norm{d_k}}
\end{equation}
In a backtracking setting, as defined in algorithm \ref{algo:backtracking}, we search for a value of $t_k$ such that:
\begin{equation}
	f(x_k) - f(x_{k+1})\geq -\alpha t_k \nabla f(x_k)^Td_k.
	\label{eq:sufficientDecreaseCond}
\end{equation}
When backtracking we have two possibilities: either $t_k=s$ satisfy inequality (\ref{eq:sufficientDecreaseCond}) or not. In the latter case it must hold:
\begin{equation}
		f(x_k)-f(x_{k}+\frac{t_k}{\beta}d_k) < -\alpha \frac{t_k}{\beta} \nabla f(x_k)^Td_k
\label{eq:sufficientDecreaseViolated}
\end{equation}
Combining the latter with inequality \ref{eq:lipschitzReworked} written for $t_k=\frac{t_k}{\beta}$ yields:
\begin{equation}
-\alpha \frac{t_k}{\beta} \nabla f(x_k)^Td_k > - \frac{t_k}{\beta}\nabla f(x_k)^Td_k\cdot \left(1+\frac{t_k}{\beta}\frac{L}{2}\frac{\nabla f(x_k)^Td_k}{\norm{\nabla f(x_k)}^2 cos^2\theta_k}\right),
\end{equation}
which in turn, being $\nabla f(x_k)^Td_k<0$ since $d_k$ is a descent direction, and $t_k,\beta>0$, leads to:
\begin{align}
	t_k &> \frac{2(\alpha-1)\beta}{L} \frac{\norm{\nabla f(x_k)}^2 cos^2\theta_k}{\nabla f(x_k)^Td_k} 	\label{eq:lowerBound}\\
	&=\frac{2(\alpha-1)\beta}{L}\frac{\nabla f(x_k)^Td_k}{\norm{d_k}^2}
\end{align}
If we impose
\begin{equation}
\delta\geq-\gamma\frac{\nabla f(x_k)^Td_k}{\norm{d_k}^2}
\label{eq:initialStepBound}
\end{equation}
where $\gamma$ is some positive constant, we can use \ref{eq:lowerBound} and \ref{eq:initialStepBound} in \ref{eq:sufficientDecreaseCond} and get:
\begin{equation}
f(x_k) - f(x_{k+1}) >\alpha\cdot \text{min}\left(\gamma,\frac{2(1-\alpha)\beta}{L}\right ) \norm{\nabla f(x_k)}^2 cos^2\theta_k.
\end{equation}
Summing over k, if $f$ is bounded below, say by $f^*$, and $\theta_k$ bounded away from 90 degrees we get:
\begin{equation}
	f(x_0)-f^* \geq \sum_{k=0}^N f(x_k) - f(x_{k+1})  = f(x_0)-f(x_N) > C\sum_{k=0}^N\norm{\nabla f(x_k)}^2.
	\label{eq:gradientNormInequality}
\end{equation}
Hence we have convergence at the same rate as gradient descent.\\\\
We have made the following assumptions along the way:
\begin{itemize}
	\item $f(\cdot)$ is bounded below
	\item $d_k$ is a descent direction bounded away from 90 degrees w.r.t $\nabla f(x_k)$
	\item the initial guess of the backtracking algorithm $\delta\geq- \gamma \frac{\nabla f(x_k)^Td_k}{\norm{d_k}^2}$ for some positive constant $\gamma$
\end{itemize}


\begin{algorithm}[!h]
	\KwData{\\
		$\delta>0$ initial step guess\\
		$\alpha,\beta \in (0,1)$
	}
	$t_k\gets \delta$\\
	\While{$f(x_k) - f(x_{k+1}) < -\alpha t_k \nabla f(x_k)^Td_k$}{
		$t_k\gets t_k\beta$
	}
	\KwRet{$t_k$}
	\caption{Backtracking algorithm.}
	\label{algo:backtracking}
\end{algorithm}

\paragraph{Convergence rate.}
From inequality (\ref{eq:gradientNormInequality}) we can immediately derive:
\begin{equation}
	C(N+1)\min_{k\in(0,N)} \norm{\nabla f(x_k)}^2  < f(x_0)-f^*,
\end{equation}
hence
\begin{equation}
\min_{k\in(0,N)} \norm{\nabla f(x_k)} < \frac{1}{\sqrt{(N+1)}}\left(\frac{f(x_0)-f^*}{C}\right)^{\frac{1}{2}},
\end{equation}
which proves that using descent directions different from the gradient still yields the same convergence rate of the gradient descent method $\mathcal{O}(\frac{1}{\sqrt{N}})$. 
 

\end{document}      



