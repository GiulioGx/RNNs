 
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{proof}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{pgf}
\usepackage{tikz}
\usepackage[noend]{algpseudocode}
\usepackage[ruled,noline,linesnumbered]{algorithm2e}
\usetikzlibrary{automata,arrows}
\let\emptyset\varnothing

\graphicspath{ {./images/} }

%%Notation:

%%vectors
\renewcommand{\vec}[1]{\boldsymbol{#1}}
%%sets
\newcommand{\set}[1]{\mathcal{#1}}
%%matrixes
\newcommand{\mat}[1]{#1}
%%norm
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
%%defeq
\newcommand{\defeq}{\triangleq}
%%
\newcommand{\net}[1]{\mathrm{#1}}
%% pair<x,y>
\newcommand{\pair}[2]{\langle#1,#2\rangle}

\title{Stochastic gradient descent}
\author{Giulio Galvan}

\begin{document}
	\maketitle
\noindent
Consider the stochastic optimization problem 
\begin{equation}
	\underset{\vec{x} \in \mathbb{R}^n}{\text{min}} f(\vec{x}) = \mathbb{E[F(\vec{x}, \vec{\xi})]},
	\label{eq:stochastic_prob}
\end{equation}
where $\vec{\xi} \in \Omega \subset \mathbb{R}^d$ is a random vector.
Suppose $f(\cdot)$ is continuous, strongly convex (with constant $c$) and there exists a compact level set of $f(\cdot)$, hence (\ref{eq:stochastic_prob}) has a unique optimal solution $\vec{x}_*$. We make the following two assumptions:
\begin{itemize}
	\item	It is possible to generate independent identically distributed samples of $\vec{\xi}$.
	\item There exists an oracle which, for a given point $(\vec{x}, \vec{\xi})$ returns a stochastic direction $D(\vec{x}, \vec{\xi})$ such that $d(\vec{x})\defeq\mathbb{E}[D(\vec{x}, \vec{\xi})]$ satisfies:
	\begin{equation}
	-(\vec{x}-\vec{x}_*)^T (f^{\prime} -d(\vec{x})) \geq -\mu L \norm{\vec{x}-\vec{x}_*}^2_2\quad \text{ for some } f^\prime \in \partial f(\vec{x}),
	\label{eq:angular_condition}
	\end{equation}
	for some $\mu \in (0,\frac{c}{L}) $, $L$ is some chosen positive constant. We assume further that there exists $M>0$ such that
	\begin{equation}
		\norm{d(\vec{x})}^2_2]\leq M^2 \quad \forall \vec{x} \in \mathbb{R}^n.
		\label{eq:gradient_bound}
	\end{equation}
\end{itemize}
Consider an algorithm defined by
\begin{equation}
	\vec{x}_{j+1} = \vec{x}_j -\gamma_j D(\vec{x}_j,\vec{\xi}_j).
	\label{eq:stochastic_algo}
\end{equation}
Each iterate $\vec{x}_j$ of such a random process is a function of the history $\vec{\xi}_{[j-1]}=(\vec{\xi}_1,\dots, \vec{\xi}_{j-1})$.

Let $A_j\defeq \norm{\vec{x}_j-\vec{x}_*}^2_2$ and $a_j\defeq\mathbb{E}[A_j]$.
From (\ref*{eq:stochastic_algo}) we get
\begin{equation}
\begin{aligned}
	A_{j+1} &= \frac{1}{2}\norm{\vec{x}_j - \gamma_jD(\vec{x}_j,\vec{\xi}_j) -\vec{x}_*}^2_2\\ 
	&= A_j +\frac{1}{2}\gamma_j^2\norm{D(\vec{x}_j,\vec{\xi}_j)}^2_2 - \gamma_j(\vec{x}_j-\vec{x}_*)^TD(\vec{x}_j,\vec{\xi}_j).
\end{aligned}
\label{eq:aj_rec}
\end{equation}
We can write:
%\begin{equation}
\begin{align}
	\mathbb{E}_{\vec{\xi}_{[j]}}[(\vec{x}_j-\vec{x}_*)^TD(\vec{x}_j,\vec{\xi}_j)] &= \mathbb{E}_{\vec{\xi}_{[j-1]}}[\mathbb{E}_{\vec{\xi}_{[j]}}[(\vec{x}_j-\vec{x}_*)^TD(\vec{x}_j,\vec{\xi}_j)]|\vec{\xi}_{[j-1]}] \label{eq:independece_1}\\ 
	&= \mathbb{E}_{\vec{\xi}_{[j-1]}}[(\vec{x}_j-\vec{x}_*)^T\mathbb{E}_{\vec{\xi}_{[j]}}[D(\vec{x}_j,\vec{\xi}_j)]|\vec{\xi}_{[j-1]}]\label{eq:independece_2} \\ 	
	&=\mathbb{E}_{\vec{\xi}_{[j-1]}}[(\vec{x}_j-\vec{x}_*)^Td(\vec{x}_j)].	\label{eq:independece}
\end{align}
Equation (\ref{eq:independece_1}) is given by the law of total expectation, (\ref{eq:independece_2}) holds because $\vec{x}_j = \vec{x}_j(\vec{\xi_{[j-1]}})$ is not function of $\vec{\xi}_j$, hence independent of it.
%\end{equation}
Using (\ref{eq:gradient_bound}) and (\ref{eq:independece}) we obtain, taking expectation on both sides of (\ref{eq:aj_rec})
\begin{equation}
	a_{j+1} \leq a_j - \gamma_j\mathbb{E}_{\vec{\xi}_{[j-1]}}[(\vec{x}_j-\vec{x}_*)^Td(\vec{x}_j)] + \frac{1}{2}\gamma_j^2M^2.
	\label{eq:aj_rec_2}
\end{equation}
Since $f(\cdot)$ is strongly convex with constant $c>0$,
\begin{equation}
	(\vec{x}-\vec{y})^T(f^\prime- g^\prime))\geq c \norm{\vec{x}-\vec{y}}^2_2, \quad \forall f^\prime\in\partial f(\vec{x}), g^\prime\in\partial f(\vec{y}).
	\label{eq:strong_convexity}
\end{equation}
By optimality of $\vec{x}_*$ we have
\begin{equation}
	(\vec{x}-\vec{x}_*)^T f^\prime \geq 0 \quad \forall \vec{x} \in \mathbb{R}^n, \forall f^\prime\in\partial f(\vec{x}_*).
	\label{eq:optimality}
\end{equation}
Inequalities (\ref{eq:strong_convexity}) and (\ref{eq:optimality}) together imply
\begin{equation}
	(\vec{x}-\vec{x}_*)^T f^\prime \geq c \norm{\vec{x}-\vec{x}_*}^2_2 \quad \forall \vec{x} \in \mathbb{R}^n, \forall f^\prime\in\partial f(\vec{x}).
\end{equation}
Adding and subtracting the oracle direction $d(\vec{x})$ we get
\begin{equation}
	(\vec{x}-\vec{x}_*)^T (f^\prime -d(\vec{x}) +d(\vec{x})) \geq c \norm{\vec{x}-\vec{x}_*}^2_2,
\end{equation}
which can be rewritten as
\begin{equation}
		(\vec{x}-\vec{x}_*)^T d(\vec{x}) \geq c \norm{\vec{x}-\vec{x}_*}^2_2 - (\vec{x}-\vec{x}_*)^T (f^\prime -d(\vec{x})).
		\label{eq:angular_inequality}
\end{equation}
From Assumption (\ref{eq:angular_condition}), and by taking expectations (from now on we will write $\mathbb{E}$ in place of $\mathbb{E}_{\vec{\xi}_{[j-1]}}$ for ease of notation) on both side of (\ref{eq:angular_inequality}), we obtain
\begin{align}
\mathbb{E}[(\vec{x}_j-\vec{x}_*)^T (\vec{x}_j)] &\geq c \mathbb{E}[\norm{\vec{x}_j-\vec{x}_*}^2_2)] - \mathbb{E}[(\vec{x}_j-\vec{x}_*)^T (f_j^\prime -d(\vec{x}_j))]\\
 &\geq c\left(1-\frac{\mu L}{c}\right) \mathbb{E}[\norm{\vec{x}_j-\vec{x}_*}^2_2]\\
 & = 2\bar{c}a_j,
\end{align}
with $\bar{c}=c(1-\frac{\mu L }{c})$ and $f_j^\prime \in \partial f(\vec{x}_j)$.
Hence from (\ref{eq:aj_rec_2}) it follows 
\begin{equation}
	a_{j+1} \leq (1-2\bar{c}\gamma_j)a_j + \frac{1}{2}\gamma_j^2M^2.
\end{equation}
Choosing now the stepsizes as $\gamma_j = \frac{\beta}{j}$ for some constant $\beta>\frac{1}{2\bar{c}}$ we get
\begin{equation}
		a_{j+1} \leq (1-2\bar{c}\gamma_j)a_j + \frac{1}{2}\frac{\beta^2M^2}{j^2}.
\end{equation}
It follows by induction that
\begin{equation}
	\mathbb{E}[\norm{\vec{x}_j - \vec{x}_*}^2_2] = 2a_j\leq \frac{Q(\beta)}{j},
\end{equation}
where 
\begin{equation}
	Q(\beta) = \max\left\{\frac{\beta^2M^2}{2\bar{c}-1},\norm{\vec{x}_1 - \vec{x}_*}^2_2 \right\}.
\end{equation}
When $\nabla f$ is Lipschitz continuous we also have
\begin{equation}
	f(\vec{x})\leq f(\vec{x}_*) + \frac{1}{2}L\norm{\vec{x} - \vec{x}_*}^2_2, \quad \forall \vec{x} \in \mathbb{R}^n,
\end{equation}
hence we can get a bound also on the function value:
\begin{equation}
	\mathbb{E}[f(\vec{x}_j)-f(\vec{x}_*)] \leq \frac{1}{2} L\, \mathbb{E}[\norm{\vec{x}_j - \vec{x}_*}^2_2] \leq \frac{1}{2}L\,Q(\beta).
\end{equation}

Assumption (\ref{eq:angular_condition}) can be further elaborated.
Let $\theta$ be the angle between $f^\prime \in\partial f(\vec{x})$ and $g(\vec{x})$. Write $\norm{g(\vec{x}_j)} = \alpha \norm{\nabla f(\vec{x}_j)}$ for some $\alpha>0$,
then
\begin{align}
	\norm{f^\prime-g(\vec{x}_j)}^2 &= \norm{f^\prime}^2 + \norm{g(\vec{x}_j)}^2 -2\norm{f^\prime}\norm{g(\vec{x}_j)}\cos\theta_j\\
	&=  \norm{f^\prime}^2(1+\alpha_j^2-2\alpha_j \cos\theta_j).
\end{align}
Hence
\begin{align}
	(\vec{x}-\vec{x}_*)^T (f^\prime -g(\vec{x})) &\leq \norm{\vec{x}-\vec{x}_*} \norm{f^\prime -g(\vec{x})}\\
	&= \norm{\vec{x}-\vec{x}_*} \norm{f^\prime}(1+\alpha_j^2-2\alpha_j \cos\theta_j)^{\frac{1}{2}}
\end{align}
Assume
\begin{equation}
\norm{f^\prime}_2\leq L \norm{\vec{x}-\vec{x}_*}_2.
\label{eq:lipschitz continuity}
\end{equation}
Note that Equation (\ref{eq:lipschitz continuity}) is implied simply by Lipschitz continuity in the differentiable case.
A sufficient condition is thus
\begin{equation}
	1+\alpha^2-2\alpha \cos\theta_j\leq \left(\frac{\mu}{L}\right)^2.
\end{equation}

\end{document}      


