\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{proof}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{pgf}
\usepackage{tikz}
\usepackage[noend]{algpseudocode}
\usepackage[ruled,noline,linesnumbered]{algorithm2e}
\usetikzlibrary{automata,arrows}
\let\emptyset\varnothing

\graphicspath{ {./images/} }

%%Notation:

%%vectors
\renewcommand{\vec}[1]{\boldsymbol{#1}}
%%sets
\newcommand{\set}[1]{\mathcal{#1}}
%%matrixes
\newcommand{\mat}[1]{#1}
%%norm
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
%%defeq
\newcommand{\defeq}{\triangleq}
%%
\newcommand{\net}[1]{\mathrm{#1}}
%% pair<x,y>
\newcommand{\pair}[2]{\langle#1,#2\rangle}

\title{On penalty terms}
\author{Giulio Galvan}

\begin{document}
	\maketitle

We have seen that:
\begin{equation}
	\frac{\partial g}{\partial \mat{W}^{rec}}= \sum_{t=1}^T\frac{\partial g_t}{\partial \vec{a}^t} \cdot \sum_{k=1}^t \frac{\partial \vec{a}^t}{\partial \vec{a}^k} \cdot \frac{\partial \vec{a}^k}{\partial \mat{W}^{rec}},
\end{equation}
where 
\begin{equation}
	\frac{\partial \vec{a}^t}{\partial \vec{a}^k} = \prod_{i=t-1}^{k}  diag(\sigma'(\vec{a}^i)) \cdot \mat{W}^{rec}
	\label{eq:temporalComponent}
\end{equation}
tends to vanish.

The idea is to develop a penalty term which express a preference for solutions where the components $\frac{\partial \vec{a}^t}{\partial \vec{a}^k}$ are far from zero, hence, learning a model which exhibit long memory.

A first attempt could be:
\begin{equation}
\Gamma \defeq \sum_{t=1}^T\sum_{k=1}^t \frac{1}{\norm{\frac{\partial \vec{a}^t}{\partial \vec{a}^k}}^2}.
\end{equation}
$\Gamma$ treats all temporal steps equally: we could assign more importance to distant temporal steps, which are the most critical, modifying $Gamma$ as follows:
\begin{equation}
\Gamma \defeq \sum_{t=1}^T\sum_{k=1}^t \frac{\sigma(t-k)}{\norm{\frac{\partial \vec{a}^t}{\partial \vec{a}^k}}^2},
\end{equation}
where $\sigma(\cdot)$ assign different weights depending on the temporal distance $t-k$, for example $\sigma(h)=exp\{h\}$

We can compute the derivative of $\Gamma$ w.r.t. $\mat{W}^{rec}$ as follows.
Let $A\defeq\frac{\partial \vec{a}^t}{\partial \vec{a}^k}$	, for ease of notation and $\norm{A}_F$ it's Frobenius norm.

\begin{align}
	\frac{\partial \norm{A}^2_F}{\partial \mat{W}_{ij}^{rec}}&=- \frac{1}{\norm{A}^2_F}\cdot \frac{\partial}{\partial w_{ij}} \sum_{xy} A_{xy}^2(w_ij)\\
	&=- \frac{1}{\norm{A}^2_F}\cdot 2\sum_{xy} A_{xy}\cdot \frac{\partial A_{xy}}{\partial w_{ij}},
\end{align}

where \begin{equation} 
A_{xy}=\frac{\partial \vec{a}_x^t}{\partial \vec{a}_y^k} = \sum_{q\in P(y)} \sum_{l \in P(q)} \hdots \sum_{h : x \in P(h)} w_{qy} \hdots w_{yh} \cdot \sigma'(a_y^k)\sigma'(a_q^{k+1}) \hdots \sigma'(a_x^{t-1})
\label{expanded_mem}
\end{equation}

For efficiency purposes and because 2nd derivatives are not always available, for example when using ReLU units, $\sigma'(a_i^k)$ can be considered constant w.r.t. to $\mat{W^{rec}}$.

\end{document}      
