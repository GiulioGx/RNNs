\subsection{Reservoir computing} 
\label{sec:reservoir}

\textit{Reservoir Computing} is a completely different paradigm to ``train'' RNNs, and in general models with complex 
dynamics, proposed independently in 2001 by Herbert Jaeger under the name \textit{Echo State 
Networks}\cite{echoStateNetworks} and by Wolfang Maas under the name \textit{Liquid Machines}\cite{liquidStateMachines}.

Methods belonging to Reservoir computing family make use of RNNs in the following way: first they \textit{randomly} 
create a RNN (i.e they assign the weight matrices), which is called the \textit{reservoir}; then they used the neurons
outputs to learn a mapping from input to target sequences.
Such methods make a strong conceptual and computational distinction between the \textit{reservoir}, and the 
\textit{readout} function.
It's important to notice that they weights of the RNNs, are not learned in any way;

The interest in such models was raised by the fact that such networks often outperformed state-of-art fully learned 
RNNs.


The several methods which falls into this category differ in they way they generate the \textit{reservoir} and the type 
of \textit{readout} mapping they make use of. \textit{Readout} functions can be simple linear functions, maybe preceded 
by a kernel expansion of the neuron output sequence, a multi-layered FFNN, etc. and they are learned in the usual way.
As for the reservoir there are several ``recipes'' for producing ``good'' ones: from fully unaware of the training set 
methods, which randomly generate the RNN aiming to provide rich dynamics, to methods which choose a RNN depending on 
the behavior of such network on the training set.
For a more detailed summary of the field please see \cite{reservoirSummary}.


\paragraph{Echo state networks}

\textit{Echo State Networks} (ESN) usually make use of a randomly generated \textit{reservoir} and of linear 
\textit{readout} function, preceded by a kernel expansion.

The ESN recipe for generating the \textit{reservoir} is to generate a \textit{big}, \textit{sparsely} and 
\textit{randomly} connected RNN. The aim of this design is to provide to the readout function signals which are 
different and loosely coupled.

The fundamental element for the ESN architecture to work is that it has to have the \textit{echo state property} that 
states that the effect of a previous (hidden) state and input on the future state should vanish gradually as time 
passes.
This is usually accomplished by controlling the spectral radius of the recurrent weight matrix $\rho(\mat{W})$.
The rule of thumb, given by ESNs inventor Jaeger is to use $\rho(W)$ close to $1$ when dealing with tasks requiring 
long memory and $\rho(W)<1$ when dealing with tasks where memory is less important.
This reminds a lot of the 
Hochreiter's conditions for vanishing/exploding gradient (section \ref{sec:vanishing}).


Another common feature of ESN is the use of a novel neuron model called \textit{leaking integrator neuron}:

\begin{equation}
 \vec{h_t} = (1-\alpha) \sigma(\mat{W}^{rec}\vec{h}_t + \mat{W}^{in}\vec{x}_t + b^{rec}) +\alpha \vec{h}_{t-1}
\end{equation}
 The parameter $\alpha$ controls the ``speed'' of the reservoir dynamics: a small value of $\alpha$ makes the reservoir 
react slowly to the input, whether a larger value would make the neurons change at faster rate.
