In this section we will investigate the expressive power of neural networks, presenting some results that motivate the use of neural networks as learning
models and underline the differences between FNNs and RNNs. 

One of the first import results regarding the expressive power of neural networks its due to Hornik et al. \cite{Hornik89} which states
\textit{``Multilayered feed foward networks with at least one hidden layer, using an arbitrary squashing function, can approximate virtually any function
of interest to any desired degree of accuracy provided sufficiently many hidden units are available''}.

To give a more formal result we need first to define what \textit{approximate to any degree of accuracy means}. This concept is captured in the following definition
\begin{defn}
 A subset S of $\mathbb{C}^n$ (continuous functions in $\mathbb{R}^n$) is said to be \textit{uniformly dense on compacta in} $\mathbb{C}^n$ if $\forall$
 compact set $K\subset \mathbb{R}^n$ holds: $\forall \epsilon >0$, $\forall g(\cdot) \in \mathbb{C}^n$ $\exists f(\cdot) \in S$ such that 
 $\underset{x \in K}{\text{sup  }} \norm{f(x)-g(x)}<\epsilon$ 
 \label{dens_compact}
\end{defn}
Hornik result is contained in theorem \ref{universal_approx}.
\begin{thm}
 For every squashing function $\sigma$, $\forall n\in \mathbb{N}$, feed forward neural
 networks with one hidden layer are a class of functions which is \textit{uniformly dense on compacta in} $\mathbb{C}^n$
\label{universal_approx}.
\end{thm}

Theorem \ref{universal_approx} extends also to Borel measurable functions, please see \cite{Hornik89} for more details.

A survey of other approaches, some of which constructive, in the sense that they actually show how to build the networks, which achieve similar results can be found in \cite{Scarselli98}.
We don't know of any results concerning ReLU activation function.

This results implies that FNN are \textit{universal approximators}, this is a strong argument for using such models in machine learning.
It is important to notice, however, that the theorem holds if we have \textit{sufficiently many} units. In practice the number of units will be bounded
by the machine capabilities and by computational time, of course greater the number of units greater will be the learning time.
\\\\Let us now turn our attention to RNNs and see how the architectural changes, namely the addition of backward links, affect the expressive power of the model.
It suffice to say that RNNs are as powerful as Turing machine. Siegelman and Sontag \cite{Siegelmann91turingcomputability} proved the existence 
of a finite neural network, with sigmoid activation function, which simulates a universal Turing machine. Hy{\"o}tyniemi \cite{Hyotyniemi96turingmachines} proved, equivalently,
that turing machine are recurrent neural network showing how to build a network, using instead ReLU activation function, that performs step by step 
all the instruction of a computer program.
Hy{\"o}tyniemi work is particularly interesting because it shows how to construct a network that simulate an algorithm written a simple language equivalent to a turing machine.
For each instruction type (increment, decrement, conditional branching, \dots) a particular setting of weights and neuron is devised allowing the net to simulate step by step the behavior of the program. 
In the program equivalent network there are a unit for each program variable and one or two, depending on the instruction type, units for each program instruction.
This is very interesting from an expressiveness point of view since it bounds the number of units we ought to use with the length of the algorithm we are trying to reproduce.

For better understanding the implications of this fact, imagine how many complex function you can express with short algorithms, for example (approximation of) fractals .
It is worth underling the difference with feed forward neural networks where a large number of units seems to be required. This seems to suggest that FFNNs and RNNs differ mainly in a manner of 
representation, where FNNs use space to define a somehow explicit mapping from input to output, RNNs use time to implicitly define an algorithm responsible for such mapping. 

This seems extremely good news, since we could simulate turing machines, hence all algorithms we can think of, using a recurrent neural network with a relatively small number of units; 
recall that for FFNN we had to suppose infinitely many units to obtain the universal approximator property.
Of course there is a pitfall: we can simulate any turing machine but we have to allow sufficiently many time steps and choose a termination criterion.
This is of course impractical, and we don't use RNNs in this way. Usually the number of time steps is chosen to be equal to the input sequence length. This of course restrict the class of algorithm we can learn with RNNs. 
In particular the class of algorithms suited to be learned by such models is that of algorithms consisting in at most one loop, i.e $\mathcal{O}(n)$ time, and constant memory. For example we can imagine to learn algorithms which, scanning the input sequences, detect particular patterns, \textit{store} them, and step by step, produce an output based on the patterns detected so far.





