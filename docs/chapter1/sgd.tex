In this section we will describe a framework based on gradient descent optimization method which can be used to train 
neural network of any kind. Such framework constitutes the core of many learning methods used in today's applications. 
Suppose we have a training set of pairs $D=\{\pair{\vec{x}^{(i)}}{\vec{y}^{(i)}}\}$ and a loss function $L(\theta)$ 
where $\theta$ represents all the parameters of the network.

A standard gradient descend would update $\theta$ at each iteration using the gradient computed on the whole training 
set, as shown below.
\begin{equation}
 \theta = \theta - \alpha \nabla_\theta L(\theta)
\end{equation}

This can be very slow or even impractical if the training set is too huge to fit in memory. Stochastic gradient descent (SGD)
overcome this problem taking into account only a part of the training set for each iteration, i.e the gradient is 
computed only on a subset $I$ of training examples. 

\begin{equation}
 \theta = \theta - \alpha \nabla_\theta L(\theta; \pair{\vec{x}^{(i)}}{\vec{y}^{(i)}}, i\in I)
 \label{eq:updateRule}
\end{equation}

The subset of training examples used for the update is called \textit{minibatch}. The number of examples for each 
minibatch is an important hyper-parameter because it affects both the speed of convergence in terms of number of 
iteration and the time needed for each iteration. At each iteration new exampled are chosen among the training set 
examples, so it could, and it always does in real applications, happen that all training set exampled have been used.
This is not a problem, since we can use the same examples over and over again. Each time we go over the entire training 
set we say we completed and \textit{epoch}. It's not unusual to iterate the learning algorithm for several epochs 
before converging.

The method is summarized in algorithm \ref{algo:sgd}.

\begin{algorithm}[]
 \KwData{\\
 \Indp
  $D=\{\pair{\vec{x}^{(i)}}{\vec{y}^{(i)}}\}$: training set\\
  $\theta_0$: initial solution \\
  $m$: size of each minibatch\\
  }
  
 \KwResult{\\
 \Indp $\theta$: solution
 }
 \BlankLine
 
 $\theta \gets \theta_0$\\
 \While{stop criterion}{
 
 $I$ $\gets$ select $m$ training example $\in D$  \\
 $\alpha \gets$ compute learning rate \\
 $\theta \gets \theta - \alpha \nabla_\theta L(\theta; \pair{\vec{x}^{(i)}}{\vec{y}^{(i)}}, i\in I)$\\
 }
\caption{Stochastic gradient descent}
\label{algo:sgd}
\end{algorithm}

In the following paragraphs we will analyze in more detail each step of the method, surveying the different alternatives 
that can be used.

\paragraph{The stop criterion}

Usually a gradient based method adopts a stop criterion which allows the procedure to stop when close enough to a (local) 
minimum, i.e $\nabla_\theta L(\theta)=0$.  This could easily lead to over-fitting, so is common practice to use a 
cross-validation technique. The most simple approach to cross-validation is to split the training set in two parts, one actually used as a pool of training examples, which will be called training set, and the other, called \textit{validation} set, used to decide when to stop.

Being $D=\{\pair{\vec{x}^{(i)}}{\vec{y}^{(i)}}, i\in(1,M)\}$ a generic subset of the dataset, we can define the \textit{error} on such set in a straightforward manner as 

\begin{equation}
 E_D = \frac{1}{M} \sum_{i=1}^M  L(\vec{x}^{(i)},\vec{y}^{(i)})
\end{equation}

Since training examples are sampled from the training set, the error on the training set will always\footnote{This is not actually true; it would in a standard gradient descent, but since we are using stochastic gradient the error could be non monotonic decreasing; however the matter here is that error mainly follow a decreasing path} be decreasing across iterations. The idea behind cross-validation is to compute, and \textit{monitor} the error on the validation set, since it's not guaranteed at all that the error would be decreasing. On the contrary, tough error will generally decrease during the first part of training, it will reach a point when it will become to increase. This is the point when we need to stop training since we are starting to over-fitting. Of course this is an ideal situation, in real applications the validation error could have a more irregular trend, but the idea holds.


DISEGNO CON LE DUE CURVE



\paragraph{Learning rate}

The parameter $\alpha$ in equation \ref{eq:updateRule}, know as \textit{step} in the optimization field, is called in AI \textit{learning rate}. Of course the strategy employed to compute such learning rate is an important ingredient in the learning method.
The most easy, and often preferred, strategy is that of \textbf{constant learning rate}; Learning rate $\alpha$ becomes another hyper-parameter of the network that can be tuned, but it remains equal, usually a very small value, across all iterations.

Another popular strategy is that of \textbf{momentum} which, in the optimization field is know as the \textit{Heavy Ball} method.
The main idea behind momentum is to accelerate progress along dimensions
in which gradient consistently point in the same direction
and to slow progress along dimensions where the sign
of the gradient continues to change. This is done by keeping
track of past parameter updates with an exponential decay as shown in equation \ref{eq:momentum}

\begin{align}
v &= \gamma v+ \alpha \nabla_\theta L(\theta; \pair{\vec{x}^{(i)}}{\vec{y}^{(i)}}, i\in I)\\
\theta &= \theta - v
\label{eq:momentum}
\end{align}

Another way of choosing the learning rate is to fix an initial value and \textbf{annealing} it, at each iteration (or epoch), according to a policy, for instance \textit{exponential} or \textit{linear} decay; the idea behind it being that, initially, when far from a minimum having a larger learning rate causes greater speed and after some iterations when approaching a minimum a smaller learning rate allows a finer refinement.

\textbf{Adaptive} methods instead choose the learning rate monitoring the objective function, hence learning rate can be reduced
or increased depending on the need, being a little more versatile than annealing methods. Of course different strategies for detection when to reduce or increase the learning rate have been devised.

Finally \textbf{line search} can be used; usually line search is used when working with (non stochastic) gradient descend or when dealing with large batches; for stochastic gradient with small batches other strategies are usually preferred.

\paragraph{Regularization}
% \paragraph{weight decay}
% \paragraph{drop out}
TODO

\paragraph{How to choose batches}

Empirical evidence has been provided that choosing a ``meaningful'' order in which examples are presented to the network can both speed the convergence and yield better solutions. Generally speaking, the network can learn faster if trained first with easier examples and then with examples with gradually increasing difficulty, as humans or animals would do. The idea was introduced by Bengio et al.\cite{curriculumLearning} in 2009, as \textit{curriculum} learning. Experiments on different curriculum strategies can be found in \cite{learningToExecute}.

