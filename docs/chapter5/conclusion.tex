We started our analysis from the gradient structure of RNNs. We showed how the recurrent nature of this model change the gradient structure in comparison to FFNNs. In particular the recurrence definition of RNNs give rise to the \textit{exploding/vanishing} gradient problem. We then analyzed some sufficient conditions for the gradient to vanish. In particular we found that the singular values of the recurrent matrix play an important role in the matter, namely, if the recurrent matrix has too small singular values the gradient vanish.

Motivated by this, we explored an initialization scheme which scales the recurrent matrix to have spectral radius larger than one. We showed that, with such an initialization, several artificial tasks that were deemed impossible to solve with SGD, for sequences longer than 20, can be solved with a non zero rate of success. This is particularly interesting because the proposed initialization scheme is extremely simple to implement and for several real application can be enough to obtain good results. 

Furthermore we proposed a novel strategy to pick a descent direction based on a combination of what we call the temporal gradients, namely, the gradients we would obtain taking the derivatives of the replicates of each variable for each time step. TODO EFFECTS. 

Finally we considered two real application. The first one is the polyphonic music prediction, where the task is, given a song, to predict at each time beat, which notes are going to be played next. The main difference with the artificial tasks is that, in this case, there is a non zero loss for each time step. We noticed that this has a huge effect on the gradient which seems not to vanish whatever the initialization. The second application, instead, comes from the recording of medical visits of patients affected by the lupus disease. The aim here is to predict whether a patience will result affected by the disease or not in the near future. Also in this scenario, since the sequences, which consist in the visits each patience has done over time, are not longer than 20, we did not observe a vanishing gradient even if the loss is defined only on the last time step. 

FUTURE WORK?