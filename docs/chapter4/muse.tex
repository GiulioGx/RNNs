 
\section{Polyphonic music prediction}

In this experiment we considered the task of polyphonic music prediction. The input sequences are polyphonic songs and the aim is, at each time step, to predict which notes are going to be played next. More precisely the input sequences are obtained from a piano roll of MIDI files where each time step corresponds to a beat (usually a quarter or an octave): each step of the sequence is hence composed of $d$ binary elements which specify if the correspondent note is played or not at the current beat, where $d$ is the number of notes that can be played.

We use as a reference the work done in \cite{BoulangerMuse} which compares several different approaches, RNNs amongst them. We use the dataset \textsl{MuseData} made available by the authors on their website
\url{http://www-etud.iro.umontreal.ca/~boulanni/icml2012}, using the provided split in train, set and validation sets.

The RNN architecture we employed is a standard RNN with \textit{tanh} units, 88 input and output units (the total number of notes from A0 to C8). We use as output function the logistic function

\begin{equation}
	F(x)=\frac{1}{1+e^{-x}}.
\end{equation}
In this way we can interpret the value of each output unit as the probability that such note is played, as for each note we have an output in $(0,1)$. We used the cross-entropy as loss function for each time step, hence, the loss for the whole sequence is given by:

\begin{equation}
	L(\vec{y}) = \frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{d} \bar{y}_i^t \log(y_i^t) + (1-\bar{y}_i^t)\log(1-y_i^t),
\end{equation}

where, as usual, $y_i^t$ and $\bar{y}_i^t$ are the predicted output and the label for note $i$ at time $t$ respectively.

An important difference compared to the artificial tasks considered before is that in this case the loss is not computed only on the last step. This has, of course, an impact on the temporal gradients as the gradient seems not to vanish in this scenario. 

We trained the network with SGD (with the anti-gradient direction) with learning rate $0.001$ and threshold on the gradient norm $3$. We explored different number of hidden units, namely 50, 100, 200, 300. We compose the batch with sequences of different lengths, up to 300 steps, cutting the original songs if they are longer than 300 steps.

Figure \ref{fig:overfitting_muse} shows the loss function computed on both train and validation sets during the training for model with different hidden units. The most obvious observation is that at a certain point the training procedure starts to over-fit and that the model with more hidden units start to over-fit sooner.

\begin{figure}
	\centering
\resizebox{13cm}{!}{
	\input{chapter4/overfitting_muse.py_.pgf}
}
\caption{Loss on both train and validation sets during training for models with different number of hidden units. The stars mark the best values on the validation sets.}
\label{fig:overfitting_muse}
\end{figure}

\begin{table}
	\centering
\begin{tabular}{c | c | c}
	hidden units & test loss & train loss \\
	50 & \textbf{6.44} & 6.20  \\
	100 & 6.53 & 6.10 \\
	200 & 6.58 & \textbf{6.03} \\
	300 & 6.82 & 6.21 \\
\end{tabular}
\caption{This table shows the loss (i.e. negative log-likelihood per time step) for both train and test sets for the models with different number of hidden units.}
\label{table:losses_n_hidden}
\end{table}

In Table \ref{table:muse_state_of_art} the test and train error for state of art approaches as well as our as shown. Although our result is worse than the state of the art we improve the best result obtained with vanilla RNNs. The reason for this (partial) success can be attributed to the choice of the hyper-parameters or the way we build up the batches.

\begin{table}
	\centering
	\begin{tabular}{c | c | c | c}
		& State of art & State of art (RNN) & Our result\\
		Train & \textbf{5.20} & 6.43 & 6.20\\
		Test & \textbf{5.60} & 6.97 & 6.44\\
	\end{tabular}
	\caption{Comparison between test and train errors with state of art results.}
	\label{table:muse_state_of_art}
	
\end{table}

