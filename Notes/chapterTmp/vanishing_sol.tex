In section \ref{vanishing_sec} were established some weak bounds on the weight matrix $\mat{W}^{rec}$ whose crossing entails the arising of
the vanishing gradient problem.

It would be nice to find some lower bounds on the weight matrix which don't allow an exponential decay of long term components. However since activation
functions can all have zero value, we cannot establish any lower bound different from zero.

However what we are really interested in is the cost of the paths which cross \textit{active} neurons because, structurally, \textit{switched off} neurons don't
contribute to the gradient. Formally we define


\begin{defn}{Active neuron}
A neuron $j$ is said to be active if $$\sigma(a_j)>\tau$$
 
\end{defn}






To overcame this problem we return to the previously introduced graph formulation where we consider each component 
$\frac{\partial \vec{a}_i^t}{\partial \vec{a}_j^k}$ as the sum path cost.



\begin{equation} 
\frac{\partial \vec{a}_i^t}{\partial \vec{a}_j^k} = \sum_{q\in P(j)} \sum_{l \in P(q)} \hdots \sum_{h : i \in P(h)} w_{qj} \hdots w_{jh} \cdot \sigma'(a_j^k)\sigma'(a_q^{k+1}) \hdots \sigma'(a_i^{t-1})
\label{expanded_mem}
\end{equation}