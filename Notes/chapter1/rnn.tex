Recurrent neural networks differ from feed foward neural networks because of the presence of recurrent connections: at least one perceptron output at a given layer $i$ is \textit{fed} to another perceptron
at a level $j<i$. This is a key difference: as we will see in the next section, rnn are not only more powerfull than ffnn but as powerfull as turing machines.


This difference in topology reflects also on the network's input and output domain, where in feed foward neural networks inputs and outputs were real valued vectors, recursive neural networks deal with
sequences of vectors, that is to say that now time is also considered. One may argue that taking time (and sequences) into consideration is some sort of limitation because it restricts our model to deal only
with a temporal inputs; that's not true, in fact we can apply rnn to non temporal data by considering space as the temporal dimension or we can feed the network with the same input for all time steps, or just
providing no input after the first time step.

FIGURE RNN


\begin{defn}[Recurrent neural network]
\label{def_rnn}
A recurrent  neural network is tuple
$$RNN\triangleq< W_{in}, W_{out}, W_{rec},\vec{b}_{out}, \vec{b}_{rec} ,\sigma(\cdot),f(\cdot)>$$
\begin{itemize}
 \item $W^{in}$ is the $r\times p$ input weight matrix
 \item $W^{rec}$ is the $r\times r$ recurrent weight matrix
 \item $W^{out}$ is the $o \times r$ output weight matrix
 \item $\vec{b}^{rec} \in \mathbb{R}^{r}$ is the bias vector for the recurrent layer
 \item $\vec{b}^{out} \in \mathbb{R}^{o}$ is the bias vector for the output layer
 \item $\sigma(\cdot): \mathbb{R}\rightarrow \mathbb{R}$ is the activation function
 \item $f(\cdot): \mathbb{R}^{o)}\rightarrow \mathbb{R}^{o}$ is the output function
\end{itemize}
$p$ is the size of input vectors, $r$ is the number of hidden units, $o$ is the size of output vectors. 
The total number of weights si given by $\mathcal{N}(W) \triangleq rp+r^2+ro$, the number of biases by $\mathcal{N}(b) \triangleq r+o $
\end{defn}

Given a $RNN$ and an input sequences $\{\vec{x}\}_t$ the output sequence $\{\vec{y}\}_t$ of the net is defined by the following:
\begin{align}
&\vec{y}_t \triangleq f(W^{out}\vec{\phi}_t + \vec{b}^{out})\\
&\vec{a}_t \triangleq W^{rec}\vec{\phi}_{t-1}+W^{in}\vec{x}_t+\vec{b}^{rec}\\
&\vec{\phi}_t \triangleq  \sigma(\vec{a}_t)
\end{align}

As we can understand from definition \ref{def_rnn}, there is only one recurrent layer, whose weights are the same for each time step, so one can asks where does the deepness of the network come from.
The answer lies in the temporal unfolding of the network, in fact if we unfold the network step by step we obtain a structure similar to the structure of a feed foward neural network. As we can observe
in figure \ref{something}, the unfolding of the network through time consist of putting identical version of the same reccurent layer on top of each other and linking the inputs of one layer to the
next one. The key difference from feed foward neural networks if, as we have already observed, that the weights in each layer are identical, and of course the additional timed inputs which are different for
each unfolded layer.

UNFOLDING FIGURE

\subsection{On expressivness of rnn}
Rnns are as powerful as turing machines
\subsection{Learning with ffnn}
We can model an optimization problem in the same way we did for feed foward neural networks, the main difference is, again, that we now deal with temporal sequences so we need a slightly different
loss function.
Given a dataset $D$:
\begin{equation}
D\triangleq\{\vec{x}_t^{(i)} \in \mathbb{R}^p, \vec{y}_t^{(i)} \in \mathbb{R}^q; t=1,...,T;  i=1,...,N\}
\end{equation}
Then we need a loss function $L_D:\mathbb{R}^{\mathcal{N}(W)+\mathcal{N}(b)} \rightarrow \mathbb{R}_{\geq 0}$ over $D$ defined as
\begin{equation}
L_D(W,b)\triangleq\frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T L_t^i(W,b) 
\end{equation}
$L_t^i$ is an arbitrary loss function for the $i^{th}$ example at time step $t$.









