Consider a $FFNN=<\vec{p},\set{W},\set{B},\sigma(\cdot),F(\cdot)>$, let $L:\mathbb{R}^{p(U)} \rightarrow \mathbb{R}$ a loss function and 
$g(\cdot):\mathbb{R}^{\mathcal{N}(\set{W})+\mathcal{N}(\set{B})} \rightarrow \mathbb{R}$ be the function defined by
$$g(\set{W},\set{B}) \triangleq L(F(a^U(\set{W},\set{B})))$$


\begin{align}
\frac{\partial g}{\partial \mat{W}^i} &= \nabla L \cdot J(F) \cdot \frac{\partial \vec{a}^U}{\partial \mat{W}^i}\\
&= \frac{\partial g}{\partial \vec{a}^U} \cdot \frac{\partial \vec{a}^U}{\partial \mat{W^i}}
\end{align}


We can easily compute $\frac{\partial g}{\partial \vec{a^U}}$ once we define $F(\cdot)$ and $L(\cdot)$, note that the weights are not involved in such computation.
Let's derive an expression for $\frac{\partial \vec{a}^U}{\partial \mat{W}^i}$.
We will start deriving such derivative using linear notation. Let's consider a single output unit $u$ and a weight $w_{lj}$ linking neuron $j$ to neuron $l$.


\begin{align}
\frac{\partial a_u}{\partial w_{lj}} &= \frac{\partial a_u}{\partial a_l} \cdot \frac{\partial a_l}{\partial w_{lj}}\\
&=\delta_{ul} \cdot \phi_j
\end{align}

where we put $$\delta_{ul} \triangleq \frac{\partial a_u}{\partial a_l}$$.

Let $P(l)$ be the set of parents of neuron $l$, formally:
\begin{equation} 
P(l) = \{ k: \exists \text{ a link between $l$ and $k$ with weight } w_{lk} \}
\end{equation}
We can compute $\delta_{ul}$ simply applying the chain rule, if we write it down in bottom-up style, as can be seen in figure \ref{deriv_arcs}, we obtain:
\begin{equation}
\delta_{ul} = \sum_{k\in P(l)} \delta_{uk} \cdot \sigma'(a_k)\cdot w_kl
\end{equation}

\tikzstyle{rnn_style}=[->,shorten >=1pt,auto,node distance=1.5cm,
  thick,
  neuron/.style={circle,fill=white!50,draw,minimum size=0.7cm,font=\sffamily\normalsize},
  missing/.style={circle,fill=white!50,draw=none,minimum size=0.7cm,font=\sffamily\Huge\bfseries},
  label/.style={node distance=1.2cm,rectangle,fill=white!50,draw=none,minimum size=0.7cm,font=\sffamily\normalsize},
  thick_edge/.style={line width=1.2pt},
  thin_edge/.style={line width=0.5pt}
  ]
\begin{figure}
 \centering
\begin{tikzpicture}[rnn_style]

  \node[neuron]    (u)       {$u$};
  
  \node[neuron]    (x1)[left of=u, below of=u]   {};
  \node[neuron]    (x2)[right of=x1]   {};
  \node[neuron]    (x3)[right of=x2]   {};
  
  \node[neuron]    (y1)[below of=x1]   {$l$};
  \node[neuron]    (y2)[right of=y1]   {};
  \node[neuron]    (y3)[right of=y2]   {};
  
  \node[neuron]    (z1)[below of=y1]   {};
  \node[neuron]    (z2)[right of=z1]   {};
  \node[neuron]    (z3)[right of=z2]   {};
  
%   \node[label]      (lu)[left of=u] {$u$};
%   \node[label]      (ll)[left of=z1] {$l$};
  
  
  \path[->] (x1) edge [thick_edge] node[]{}   (u)
	    (x2) edge [thick_edge]   (u)
	    (x3) edge [thick_edge]   (u)
	    (y1) edge [thick_edge]   (x1)
	    (y1) edge [thick_edge]   (x2)
	    (y1) edge [thick_edge]   (x3)
	    (y1) edge [thin_edge]   (x2)
	    (y2) edge [thin_edge]   (x3)
	    (y3) edge [thin_edge]   (x1)
	    (y1) edge [thin_edge]   (x3)
	    (y2) edge [thin_edge]   (x2)
	    (y3) edge [thin_edge]   (x1)
	  
	    (z1) edge [thin_edge]   (y1)
	    (z1) edge [thin_edge]   (y2)
	    (z1) edge [thin_edge]   (y3)
	    (z1) edge [thin_edge]   (y2)
	    (z2) edge [thin_edge]   (y3)
	    (z3) edge [thin_edge]   (y1)
	    (z1) edge [thin_edge]   (y3)
	    (z2) edge [thin_edge]   (y2)
	    (z3) edge [thin_edge]   (y1);


\end{tikzpicture}
\caption{Nodes involved in $\frac{\partial a_u }{\partial a_l}$}
\label{deriv_arcs}
\end{figure}

The derivates with respect to biases are compute in the same way:

\begin{align}
\frac{\partial a_u}{\partial b_{l}} &= \frac{\partial a_u}{\partial a_l} \cdot \frac{\partial a_l}{\partial b_{l}}\\
&=\delta_{ul} \cdot 1
\end{align}



In matrix notation we can rewrite the previous equations as:
\begin{equation}
 \frac{\partial a^U}{\partial \mat{W}^i} = \frac{\partial a^U}{\partial a^{i+1}} \cdot \frac{\partial a^{i+1}}{\partial \mat{W}^i}
\end{equation}


\begin{equation}
\frac{\partial a^{i+1}}{\partial \mat{W}_j^i} =
 \begin{bmatrix}
   \phi_1^{i}    & 0                & \cdots      & \cdots       & 0  \\
   0               & \phi_2^{i}     & \cdots      & \cdots       & 0  \\
   \vdots          & \vdots           & \ddots      & \vdots       &\vdots\\
   0               & \cdots           & \cdots      & \cdots       & \phi^{i}_{p(i-1)}
\end{bmatrix}
\end{equation}

\begin{equation}
\frac{\partial a^U}{\partial a^{i+1}} \triangleq \Delta^i = \Delta^{i+1}  \cdot W^{i} \cdot diag(\sigma'(\vec{a}^{i}))
\end{equation}

where
\begin{equation}
diag(\sigma'(\vec{a}^{i+i})) =
 \begin{bmatrix}
   \sigma'(a^{i+1}_1)    & 0                & \cdots      & \cdots       & 0  \\
   0                     & \sigma'(a^{i+1}_2)     & \cdots      & \cdots       & 0  \\
   \vdots                & \vdots           & \ddots      & \vdots       &\vdots\\
   0                     & \cdots           & \cdots      & \cdots       &\sigma'(a^{i+1}_{p(i+1)})
\end{bmatrix}
\end{equation}

\begin{align}
\frac{\partial a^U}{\partial b^i} &= \frac{\partial a^U}{\partial a^i} \cdot \frac{\partial a^i}{\partial b^i}\\
&= \Delta^{i} \cdot Id
\end{align}
