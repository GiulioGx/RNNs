\subsection{Gradient}

We can compute partial derivatives with respect to a single weight $w_{lj}$, using simply the chain rule, as 

$$\frac{\partial L}{\partial w_{lj}}=\frac{\partial L}{\partial a_l} \cdot \frac{\partial a_l}{\partial w_{lj}}=\delta_l \cdot \phi_j$$
where we put

\begin{equation}
\delta_l \triangleq \frac{\partial L}{\partial a_l}
\end{equation}



So we can easily compute $\delta_u = \frac{\partial L^{(i)}}{\partial a_u} $ for each output unit $u$ once we choose a differentiable loss function; note
that we don't need the weights for such a computation. 

Let $P(l)$ be the set of parents of neuron $l$, formally:
\begin{equation} 
P(l) = \{ k: \exists \text{ a link between $l$ and $k$ with weight } w_{lk} \}
\end{equation}

Again, simply using the chain rule, we can write, for each non output unit $l$:

\begin{equation}
\label{loss_deriv}
\delta_l = \sum_{k\in P(l)} \frac{\partial L^{(i)}}{\partial a_k} \cdot \frac{\partial a_k}{\partial a_l}= \sum_{k\in P(l)} \delta_k \cdot 
\frac{\partial a_k}{\partial \phi_l} \cdot \frac{\partial \phi_l}{\partial a_l} = \sum_{k\in P(l)} \delta_k \cdot 
w_{kl} \cdot \sigma'(a_l)
\end{equation}


For output units instead we can compute $\delta_u = \frac{\partial L^{(i)}}{\partial a_u} $ directly once we define the loss function.

For biases variables partial derivatives are simply given by:
$$\frac{\partial L}{\partial b_{l}}=\frac{\partial L}{\partial a_l} \cdot \frac{\partial a_l}{\partial w_{l}}=\delta_l \cdot 1$$




In the following we rewrite the previously derived equations in matrix notation.
Let us recall that the weight matrix for the $i^{th}$ layer is the $p(i) \times p(i-1)$ matrix whose elements $w_{l,k}$ are the weights of the arcs which link neuron $k$ from level $i-1$ to neuron $l$ from level $i$, where
$p(i)$ is the number of neuron layer $i$ is composed of.


We can rewrite equation \ref{loss_deriv} in matrix notation as:

\begin{equation}
 \frac{\partial L}{\partial W^i} = \frac{\partial L}{\partial \vec{a}^{i}} \cdot\Big(\frac{\partial \vec{a}^{i}}{\partial W^i}\Big)^T =
 \Delta^i \cdot (\vec{\phi}^{i-1})^T
\end{equation}

where
\begin{equation}
\Delta^i  \triangleq  \frac{\partial L}{\partial \vec{a}^{i}} 
\end{equation}

\begin{equation}
 \Delta^i = \big(W^{i+1}\big)^T \cdot \Delta^{i+1} \circ \sigma(\Delta^i)
\end{equation} 

\begin{equation}
 \frac{\partial L}{\partial b^i} = \frac{\partial L}{\partial \vec{a}^{i}} \cdot\Big(\frac{\partial \vec{a}^{i}}{\partial b^i}\Big)^T =
 \Delta^i \cdot Id
\end{equation} 
