
A feed foward neural network is an artificial neural network in which there are no cycles, that is to say each layer output is \textit{fed} to the next one and connections to any earlier layer are not possible. 


\begin{defn}[Feed foward neural network]
\label{def_ffnn}
A feed foward neural network is tuple
$$FFNN\triangleq<\vec{p},W,b,\sigma(\cdot),F(\cdot)>$$
\begin{itemize}
 \item $\vec{p} \in \mathbb{N}^U$ is the vector whose elements $p(k)$ are the number of neurons of layer $k$; $U$ is the number of layers
 \item $W\triangleq \{W^k_{p(k+1) \times p(k)}, k=1,...,U \}$ is the set of weight matrixes of each layer
 \item $b \triangleq \{\vec{b}^k \in \mathbb{R}^{p(k)}, k=1,...,U \} $ is the set of bias vectors
 \item $\sigma(\cdot): \mathbb{R}\rightarrow \mathbb{R}$ is the activation function
 \item $F(\cdot): \mathbb{R}^{p(U)}\rightarrow \mathbb{R}^{p(U)}$ is the output function
\end{itemize}
\end{defn}

\begin{remark}{}
Given a FFNN:
\begin{itemize}
 \item The number of output units is $p(U)$
 \item The number of input units is $p(1)$
 \item The total number of weights is $\mathcal{N}(W) \triangleq \sum_{k=1}^U p(k+1)p(k)$
 \item The total number of biases is $\mathcal{N}(b) \triangleq \sum_{k=1}^U p(k)$
\end{itemize}
\end{remark}

\begin{defn}[Output of a FFNN]
Given a $FFNN$ and an input vector $\vec{x} \in \mathbb{R}^{p(1)}$ the output $\vec{y} \in \mathbb{R}^U$ of the net is defined by the following:

\begin{align}
&\vec{y}=F(\vec{a}^{U}) &\\
&\vec{\phi}^{i} \triangleq \sigma(\vec{a}_{i}), & i=1,...,U\\
&\vec{a}^{i} \triangleq W^{i+1} \cdot \vec{\phi}^{i-1} +\vec{b}^i  & i=1,...,U\\
&\vec{\phi}^{0} \triangleq \vec{x} &
\end{align}
\end{defn}

\subsection{On expressivness of ffnn}
\subsection{Learning with ffnn}

As we have seen in the previous section a model sush as $FFNN$ can approximate arbitrary well any smooth function, so a natural application of feed foward neural networks is machine learning.
To model an optimization problem we first need to define a dataset $D$ as 
\begin{equation}
D\triangleq\{x^{(i)} \in \mathbb{R}^p, y^{(i)} \in \mathbb{R}^q,  i=1,...,N\}
\end{equation}
Then we need a loss function $L_D:\mathbb{R}^{\mathcal{N}(W)+\mathcal{N}(b)} \rightarrow \mathbb{R}_{\geq 0}$ over $D$ defined as
\begin{equation}
L_D(W,b)\triangleq\frac{1}{N}\sum_{i=1}^N L(\vec{x}^{(i)},\vec{y}^{(i)}) 
\end{equation}
$L(\vec{x},\vec{y}):\mathbb{R}^{p(U)} \rightarrow \mathbb{R}$ is an arbitrary loss function computed on the $i^{th}$ example. Note that $\vec{y}$ is the output of the
networks, so it depends on $(W,b)$


The problem is then fo find a $FFNN$ which minimize $L_D$. As we have seen feed foward neural network allow for large customization: the only variables in the optimization problem are the weights, the other
parameters are said \textit{hyper-parameters} and are determined \textit{a priori}. Usually the output function is choosen depending on the output, for instance for multi-way classification
is generally used the softmax function, for regression a simple identity function.

The activation function $\sigma(\cdot)$ is often choosen from:
\begin{equation}
 sigmoid(x)=\frac{1}{1-e^{-x}}
\end{equation}
\begin{equation}
 tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
\end{equation}

\begin{equation}
  ReLU(x)=\begin{cases}
    x, & \text{if $x>0$}.\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}

For what concerns the number of layers and the number of units per layers they are choosen relying on experience or performing some kind of hyper-parameter tuning, which usually consists on training nets
with some different configurations of such parameters and choosing the best one.

Once we have selected the values for all hyper-paramters the optimization problem becomes:

\begin{align}
& \underset{W,b}{\text{min}}
& & L_D(W,b) \\
\end{align}


\input{chapter1/fnn_grad_revised}

