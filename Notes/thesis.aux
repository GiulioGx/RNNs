\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{UKenglish}
\@writefile{toc}{\select@language{UKenglish}}
\@writefile{lof}{\select@language{UKenglish}}
\@writefile{lot}{\select@language{UKenglish}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Artificial neural networks}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}A family of models}{5}{section.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Artificial neural network example}}{5}{figure.1.1}}
\newlabel{fully_connected}{{1.1}{5}{Artificial neural network example}{figure.1.1}{}}
\newlabel{def_perceptron_1}{{1.1}{6}{A family of models}{equation.1.1.1}{}}
\newlabel{def_perceptron_2}{{1.2}{6}{A family of models}{equation.1.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Neuron model}}{6}{figure.1.2}}
\newlabel{neuron_model}{{1.2}{6}{Neuron model}{figure.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{The activation function}{6}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{The bias term}{7}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Layered view of the net}{7}{section*.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Layered structure of an artificial neural network}}{8}{figure.1.3}}
\newlabel{layered_nnet}{{1.3}{8}{Layered structure of an artificial neural network}{figure.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Feed foward neural networks}{8}{section.1.2}}
\newlabel{def_ffnn}{{1}{8}{Feed forward neural network}{defn.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Learning with FFNNs}{9}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Gradient}{10}{subsection.1.2.2}}
\newlabel{loss_over_x_i}{{1.14}{10}{Gradient}{equation.1.2.14}{}}
\citation{Rumelhart86}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Nodes and edges involved in $\frac  {\partial a_u }{\partial a_l}$}}{11}{figure.1.4}}
\newlabel{deriv_arcs}{{1.4}{11}{Nodes and edges involved in $\frac {\partial a_u }{\partial a_l}$}{figure.1.4}{}}
\newlabel{fnn_delta}{{1.25}{11}{Gradient}{equation.1.2.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Backpropagation}{11}{section*.5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Recurrent neural networks}{12}{section.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces $\mathrm  {RNN}$ model}}{12}{figure.1.5}}
\newlabel{rnn_model}{{1.5}{12}{$\net {RNN}$ model}{figure.1.5}{}}
\newlabel{def_rnn}{{3}{13}{Recurrent neural network}{defn.3}{}}
\newlabel{def_rnn_output}{{4}{13}{Output of a $\net {RNN}$}{defn.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Unfolding of a $\mathrm  {RNN}$}}{14}{figure.1.6}}
\newlabel{rnn_unfolding}{{1.6}{14}{Unfolding of a $\net {RNN}$}{figure.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Learning with RNNs}{14}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Gradient}{15}{subsection.1.3.2}}
\newlabel{sum_over_time}{{1.37}{15}{Gradient}{equation.1.3.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Nodes involved in $\frac  {\partial a^t_u }{\partial a^k_l}$}}{16}{figure.1.7}}
\newlabel{deriv_arcs_rnn}{{1.7}{16}{Nodes involved in $\frac {\partial a^t_u }{\partial a^k_l}$}{figure.1.7}{}}
\citation{Williams90anefficient}
\newlabel{rnn_delta}{{1.45}{17}{Gradient}{equation.1.3.45}{}}
\@writefile{toc}{\contentsline {paragraph}{Backpropagation through time (BPTT)}{17}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}The vanishing and exploding gradient problem}{17}{subsection.1.3.3}}
\citation{Hochreiter95longshort-term}
\newlabel{memory_eq}{{1.48}{18}{The vanishing and exploding gradient problem}{equation.1.3.48}{}}
\@writefile{toc}{\contentsline {paragraph}{Hochreiter Analysis: Weak upper bound}{18}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{Upper bound using singular values}{19}{section*.8}}
\newlabel{memory_eq}{{1.55}{19}{Upper bound using singular values}{equation.1.3.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces The cost for a path from neuron $2$ at time $k$ to neuron $1$ at time $t$ is $w_{12}w_{31}w_{13}\mathinner {\ldotp \ldotp \ldotp }w_{11}\cdot \sigma _2^k \sigma _1^{k+1}\sigma _3^{k+2} \mathinner {\ldotp \ldotp \ldotp }\sigma _1^{t-1} $ }}{20}{figure.1.8}}
\newlabel{gradient_path_cost}{{1.8}{20}{The cost for a path from neuron $2$ at time $k$ to neuron $1$ at time $t$ is $w_{12}w_{31}w_{13}\hdots w_{11}\cdot \sigma _2^k \sigma _1^{k+1}\sigma _3^{k+2} \hdots \sigma _1^{t-1} $}{figure.1.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Explaining the problem using the network's graph}{20}{section*.9}}
\newlabel{expanded_mem}{{1.58}{20}{Explaining the problem using the network's graph}{equation.1.3.58}{}}
\@writefile{toc}{\contentsline {paragraph}{The ReLU case}{20}{section*.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces The cost for an enabled path from neuron $2$ at time $k$ to neuron $1$ at time $t$ is $w_{12}w_{31}w_{13}\mathinner {\ldotp \ldotp \ldotp }w_{11}$ }}{21}{figure.1.9}}
\newlabel{gradient_path_cost_relu}{{1.9}{21}{The cost for an enabled path from neuron $2$ at time $k$ to neuron $1$ at time $t$ is $w_{12}w_{31}w_{13}\hdots w_{11}$}{figure.1.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Poor solutions}{21}{section*.11}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Activation functions and gradient}{22}{section.1.4}}
\@writefile{toc}{\contentsline {paragraph}{Sigmoid}{22}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{Tanh}{22}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces sigmoid and it's derivative}}{23}{figure.1.10}}
\newlabel{sigmoid_plot}{{1.10}{23}{sigmoid and it's derivative}{figure.1.10}{}}
\@writefile{toc}{\contentsline {paragraph}{ReLU}{23}{section*.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces tanh and it's derivative}}{24}{figure.1.11}}
\newlabel{tanh_plot}{{1.11}{24}{tanh and it's derivative}{figure.1.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces ReLU and it's derivative}}{24}{figure.1.12}}
\newlabel{relu_plot}{{1.12}{24}{ReLU and it's derivative}{figure.1.12}{}}
\citation{Hornik89}
\citation{Hornik89}
\citation{Scarselli98}
\citation{Siegelmann91turingcomputability}
\citation{Hyotyniemi96turingmachines}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}On expressiveness}{25}{section.1.5}}
\newlabel{dens_compact}{{6}{25}{}{defn.6}{}}
\newlabel{universal_approx}{{1}{25}{}{thm.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Notation}{27}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibdata{biblio}
\bibcite{Hochreiter95longshort-term}{1}
\bibcite{Hornik89}{2}
\bibcite{Hyotyniemi96turingmachines}{3}
\bibcite{Rumelhart86}{4}
\bibcite{Scarselli98}{5}
\bibcite{Siegelmann91turingcomputability}{6}
\bibcite{Williams90anefficient}{7}
\bibstyle{plain}
