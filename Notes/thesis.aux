\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{UKenglish}
\@writefile{toc}{\select@language{UKenglish}}
\@writefile{lof}{\select@language{UKenglish}}
\@writefile{lot}{\select@language{UKenglish}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Artificial neural networks}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}A family of models}{5}{section.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Artificial neural network example}}{6}{figure.1.1}}
\newlabel{fully_connected}{{1.1}{6}{Artificial neural network example}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Neuron model}}{6}{figure.1.2}}
\newlabel{neuron_model}{{1.2}{6}{Neuron model}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Layered structure of an artificial neural network}}{7}{figure.1.3}}
\newlabel{layered_nnet}{{1.3}{7}{Layered structure of an artificial neural network}{figure.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Feed foward neural networks}{7}{section.1.2}}
\newlabel{def_ffnn}{{1}{7}{Feed foward neural network}{defn.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}On expressivness of ffnn}{8}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Learning with ffnn}{8}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Gradient}{9}{subsection.1.2.3}}
\citation{Rumelhart86}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Nodes involved in $\frac  {\partial a_u }{\partial a_l}$}}{10}{figure.1.4}}
\newlabel{deriv_arcs}{{1.4}{10}{Nodes involved in $\frac {\partial a_u }{\partial a_l}$}{figure.1.4}{}}
\newlabel{fnn_delta}{{1.23}{10}{Gradient}{equation.1.2.23}{}}
\@writefile{toc}{\contentsline {paragraph}{Backpropagation}{11}{section*.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Recurrent neural networks}{11}{section.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Rnn model}}{11}{figure.1.5}}
\newlabel{rnn_model}{{1.5}{11}{Rnn model}{figure.1.5}{}}
\newlabel{def_rnn}{{3}{12}{Recurrent neural network}{defn.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Unfolding of a rnn}}{13}{figure.1.6}}
\newlabel{rnn_unfolding}{{1.6}{13}{Unfolding of a rnn}{figure.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}On expressivness of rnn}{13}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Learning with ffnn}{14}{subsection.1.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Gradient}{14}{subsection.1.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Nodes involved in $\frac  {\partial a^t_u }{\partial a^k_l}$}}{15}{figure.1.7}}
\newlabel{deriv_arcs_rnn}{{1.7}{15}{Nodes involved in $\frac {\partial a^t_u }{\partial a^k_l}$}{figure.1.7}{}}
\newlabel{sum_over_time}{{1.34}{15}{Gradient}{equation.1.3.34}{}}
\citation{Williams90anefficient}
\newlabel{rnn_delta}{{1.42}{16}{Gradient}{equation.1.3.42}{}}
\@writefile{toc}{\contentsline {paragraph}{Backpropagation through time (BPTT)}{16}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}The vanishing and exploding gradient problem}{16}{subsection.1.3.4}}
\citation{Hochreiter95longshort-term}
\newlabel{memory_eq}{{1.45}{17}{The vanishing and exploding gradient problem}{equation.1.3.45}{}}
\@writefile{toc}{\contentsline {paragraph}{Hochreiter Analysis: Weak upper bound}{17}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{Explaining the problem using network's graph}{18}{section*.5}}
\newlabel{expanded_mem}{{1.51}{18}{Explaining the problem using network's graph}{equation.1.3.51}{}}
\@writefile{toc}{\contentsline {paragraph}{The ReLU case}{18}{section*.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces The cost for a path from neuron $2$ at time $k$ to neuron $1$ at time $t$ is $w_{12}w_{31}w_{13}\mathinner {\ldotp \ldotp \ldotp }w_{11}\cdot \sigma _2^k \sigma _1^{k+1}\sigma _3^{k+2} \mathinner {\ldotp \ldotp \ldotp }\sigma _1^{t-1} $ }}{19}{figure.1.8}}
\newlabel{gradient_path_cost}{{1.8}{19}{The cost for a path from neuron $2$ at time $k$ to neuron $1$ at time $t$ is $w_{12}w_{31}w_{13}\hdots w_{11}\cdot \sigma _2^k \sigma _1^{k+1}\sigma _3^{k+2} \hdots \sigma _1^{t-1} $}{figure.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces The cost for an enabled path from neuron $2$ at time $k$ to neuron $1$ at time $t$ is $w_{12}w_{31}w_{13}\mathinner {\ldotp \ldotp \ldotp }w_{11}$ }}{19}{figure.1.9}}
\newlabel{gradient_path_cost_relu}{{1.9}{19}{The cost for an enabled path from neuron $2$ at time $k$ to neuron $1$ at time $t$ is $w_{12}w_{31}w_{13}\hdots w_{11}$}{figure.1.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Activation functions and gradient}{20}{section.1.4}}
\@writefile{toc}{\contentsline {paragraph}{Sigmoid}{20}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{ReLU}{20}{section*.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces sigmoid and it's derivative}}{21}{figure.1.10}}
\newlabel{sigmoid_plot}{{1.10}{21}{sigmoid and it's derivative}{figure.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces ReLU and it's derivative}}{21}{figure.1.11}}
\newlabel{relu_plot}{{1.11}{21}{ReLU and it's derivative}{figure.1.11}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Notation}{23}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Derivative with respect to vector}{23}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Derivative with respect to matrix}{23}{section*.10}}
\bibdata{biblio}
\bibcite{Hochreiter95longshort-term}{1}
\bibcite{Rumelhart86}{2}
\bibcite{Williams90anefficient}{3}
\bibstyle{plain}
