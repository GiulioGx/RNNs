\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{proof}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usetikzlibrary{automata,arrows}
\let\emptyset\varnothing

\graphicspath{ {./images/} }

\title{On Gradient}
\author{Giulio Galvan}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{How to compute gradient}
\subsection{Backpropagation}

First of all we need to define a loss function over the training data, so we define a dataset as 

\begin{equation}
D=\{x^{(i)} \in \mathbb{R}^p, y^{(i)} \in \mathbb{R}^q,  i\in[1,N]\}
\end{equation}

and the loss function as
\begin{equation}
L(W)=\frac{1}{N}\sum_{i=1}^N L^{(i)}(W) 
\end{equation}

where $W$ is represents all the weights of the net.
The network is defined by
\begin{equation}
a_l \triangleq \sum_j w_{lj}\phi_j
\end{equation}

\begin{equation}
\phi_l \triangleq \sigma(a_l)
\end{equation}


where $w_{lj}$ is the weight of the connection between neuron $j$ and neuron $l$ and $\sigma$ is the non linear activation function


\tikzstyle{rnn_style}=[->,shorten >=1pt,auto,node distance=1.5cm,
  thick,
  neuron/.style={circle,fill=white!50,draw,minimum size=0.7cm,font=\sffamily\Large\bfseries},
  missing/.style={circle,fill=white!50,draw=none,minimum size=0.7cm,font=\sffamily\Huge\bfseries},
  label/.style={node distance=1.2cm,rectangle,fill=white!50,draw=none,minimum size=0.7cm,font=\sffamily\normalsize},
  layer/.style={rectangle,fill=white!50,draw,minimum width=1.5cm,font=\sffamily\normalsize},
  loopStyle/.style={in=120,out=60, distance=2.5cm},]
\begin{figure}[h!]
 \centering
\begin{tikzpicture}[rnn_style]

  \node[neuron]    (x0)       {};
  \node[neuron]    (x1)[below of=x0]   {};
  \node[neuron]    (x2)[below of=x1]   {};
  \node[missing]   (x3)[below of=x2]   { $\hdots$};
  \node[neuron]    (xn)[below of=x3]   {};
  
  \node[label]    (u0)[left of=x0]   {$u_0[t]$};
  \node[label]    (u1)[left of=x1]   {$u_1[t]$};
  \node[label]    (u2)[left of=x2]   {$u_2[t]$};
  \node[label]    (un)[left of=xn]   {$u_n[t]$};
  
  
  \node[layer] (hl)[ right of=x2,node distance=2.5cm] {$\Sigma$};
  \node[layer] (ol)[right of=hl,node distance=2.5cm] {$\sigma$};
  
  \node[label]  (phi)[right of=ol] {$\phi_l$};
   
  
  \path[->] (x0) edge [] node[]{$W_{in}$}   (hl)
	    (x1) edge []   (hl)
	    (x2) edge []   (hl)
	    (xn) edge []   (hl)
	    (u0) edge []   (x0)
	    (u1) edge []   (x1)
	    (u2) edge []   (x2)
	    (un) edge []   (xn)
	    (hl) edge []   (ol)
	    (ol) edge []   (phi);

\end{tikzpicture}
\caption{Modello per RNN}
\label{rnn_model}
\end{figure}

So we can compute partial derivatives with respect to a single weight $w_{lj}$, using simply the chain rule, as 

$$\frac{\partial L^{(i)}}{\partial w_{lj}}=\frac{\partial L^{(i)}}{\partial a_l} \cdot \frac{\partial a_l}{\partial w_{lj}}=\delta_l \cdot \phi_j$$
where we put

\begin{equation}
\delta_l \triangleq \frac{\partial L^{(i)}}{\partial a_l}
\end{equation}



So we can easily compute $\delta_u = \frac{\partial L^{(i)}}{\partial a_u} $ for each output unit $u$ once we choose a differentiable loss function; note
that we don't need the weights for such a computation. 

Let $P(l)$ be the set of parents of neuron $l$, formally:
\begin{equation} 
P(l) = \{ k: \exists \text{ a link between $l$ and $k$ with weight } w_{lk} \}
\end{equation}

Again, simply using the chain rule, we can write, for each non output unit $l$:

\begin{equation}
\label{loss_deriv}
\delta_l = \sum_{k\in P(l)} \frac{\partial L^{(i)}}{\partial a_k} \cdot \frac{\partial a_k}{\partial a_l}= \sum_{k\in P(l)} \delta_k \cdot 
\frac{\partial a_k}{\partial \phi_l} \cdot \frac{\partial \phi_l}{\partial a_l} = \sum_{k\in P(l)} \delta_k \cdot 
w_{kl} \cdot \sigma'(a_l)
\end{equation}


For output units instead we can compute $\delta_u = \frac{\partial L^{(i)}}{\partial a_u} $ directly once we define the loss function.

\subsection{Backpropagation matrix notation}

Here we rewrite the previously derived equations in matrix notation.

Let us define the weight matrix $W_i \in \mathbb{R}_{(p(i),p(i-1))}$, whose element $W_{i,j}$ is the weight of the arc which links neuron j from level i-1 to neuron i from level i, where
$p(i)$ is the neuron number for $i^{th}$ level.

\begin{equation}
 \overrightarrow{\phi}_1 \triangleq \overrightarrow{x}
\end{equation}

\begin{equation}
 \overrightarrow{a}_{i+1} \triangleq W_{i+1} \cdot \overrightarrow{\phi}_i
\end{equation}

\begin{equation}
 \overrightarrow{\phi}_{i+1} \triangleq \sigma(\overrightarrow{a}_{i+1})
\end{equation}

where $\sigma(\cdot)$ is the non-linear activation function and it's applied element by element.
We can rewrite equation \ref{loss_deriv} in matrix notation as:

\begin{equation}
 \frac{\partial L}{\partial W_i} = \frac{\partial L}{\partial \overrightarrow{a}_{i}} \cdot\frac{\partial \overrightarrow{a}_{i}}{\partial W_i}^T =
 \Delta_i \cdot \overrightarrow{\phi}_{i-1}^T
\end{equation}

where
\begin{equation}
\Delta_i  \triangleq  \frac{\partial L}{\partial \overrightarrow{a}_{i}} 
\end{equation}

\begin{equation}
 \Delta_i = W_{i+1}^T \cdot \Delta_{i+1} \circ \sigma(\Delta_i)
\end{equation}




\newpage
\nocite{*}		 % Mostra in bibliografia anche gli oggetti non citati 
\bibliography{biblio}{}
\bibliographystyle{plain}


\end{document}     