\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{proof}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usetikzlibrary{automata,arrows,decorations.pathreplacing}
\let\emptyset\varnothing

\graphicspath{ {./images/} }

\title{On Gradient}
\author{Giulio Galvan}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{How to compute gradient}
\subsection{Backpropagation}

First of all we need to define a loss function over the training data, so we define a dataset as 

\begin{equation}
D=\{x^{(i)} \in \mathbb{R}^p, y^{(i)} \in \mathbb{R}^q,  i\in[1,N]\}
\end{equation}

and the loss function $L:\mathbb{R}^U \rightarrow \mathbb{R}$ as
\begin{equation}
L(w)=\frac{1}{N}\sum_{i=1}^N L(w) 
\end{equation}

where $w\in \mathbb{R}^U$ represents all the weights of the net.

\tikzstyle{rnn_style}=[->,shorten >=1pt,auto,node distance=1.5cm,
  thick,
  neuron/.style={circle,fill=white!50,draw,minimum size=0.7cm,font=\sffamily\Large\bfseries},
  missing/.style={circle,fill=white!50,draw=none,minimum size=0.7cm,font=\sffamily\Huge\bfseries},
  label/.style={node distance=1.2cm,rectangle,fill=white!50,draw=none,minimum size=0.7cm,font=\sffamily\normalsize},
  layer/.style={rectangle,fill=white!50,draw,minimum width=4cm,font=\sffamily\normalsize},
  loopStyle/.style={in=120,out=60, distance=2.5cm},]
\begin{figure}[h!]
 \centering
\begin{tikzpicture}[rnn_style]

  \node[neuron]    (x0)       {};
  \node[neuron]    (x1)[right of=x0]   {};
  \node[neuron]    (x2)[right of=x1]   {};
  \node[missing]   (x3)[right of=x2]   { $\hdots$};
  \node[neuron]    (xn)[right of=x3]   {};
  
  \node[label]    (u0)[below of=x0]   {$x_0$};
  \node[label]    (u1)[below of=x1]   {$x_1$};
  \node[label]    (u2)[below of=x2]   {$x_2$};
  \node[label]    (un)[below of=xn]   {$x_p$};
  
  

    
  \node[layer] (hl)[above of=x2,node distance=1.8cm] {First hidden layer};
  \node[missing] (hls)[above of=hl,node distance=1.5cm]{$\hdots$};
  \node[layer] (ol)[above of=hls,node distance=1.5cm] {Output layer};
  
  \draw[decorate,decoration={brace,raise=6pt,amplitude=10pt}, thick]
   (ol.south east)--(hl.north east);
   
  \node[label]    (hls_label)[right of=hls,node distance=4.5cm]   {hidden layers};

  
  \node[neuron] (o1) at (0,6.5) {};
  \node[neuron] (o2)[right of=o1] {};
  \node[neuron] (o3)[right of=o2] {};
  \node[missing](o4)[right of=o3] {$\hdots$};
  \node[neuron] (on)[right of=o4] {};
  
  
  \node[label]    (y0)[above of=o1]   {$y_0$};
  \node[label]    (y1)[above of=o2]   {$y_1$};
  \node[label]    (y2)[above of=o3]   {$y_2$};
  \node[label]    (yn)[above of=on]   {$y_q$};
  
  
  \path[->] (x0) edge [] node[]{}   (hl)
	    (x2) edge []   (hl)
	    (x1) edge []   (hl)
	    (xn) edge []   (hl)
	    (u0) edge []   (x0)
	    (u1) edge []   (x1)
	    (u2) edge []   (x2)
	    (un) edge []   (xn)
	    (ol) edge []   (o1)
	    (ol) edge []   (o2)
	    (ol) edge []   (o3)
	    (ol) edge []   (on)
	    (o1) edge []   (y0)
	    (o2) edge []   (y1)
	    (o3) edge []   (y2)
	    (on) edge []   (yn)
	    (hl) edge []  node[]{} (hls)
	    (hls) edge []  node[]{} (ol);


\end{tikzpicture}
\caption{Feed forward neural network model}
\label{ffnnet}
\end{figure}




The network is composed of several layers as show in figure \ref{ffnnet},each layer consists in several neuron defined, as show in figure \ref{neuron_model}, by
\begin{equation}
a_l \triangleq \sum_j w_{lj}\phi_j
\end{equation}

\begin{equation}
\phi_l \triangleq \sigma(a_l)
\end{equation}


where $w_{lj}$ is the weight of the connection between neuron $j$ and neuron $l$ and $\sigma$ is the non linear activation function.


\tikzstyle{nn_style}=[->,shorten >=1pt,auto,node distance=1.5cm,
  thick,
  neuron/.style={circle,fill=white!50,node distance=1cm,draw,minimum size=0.7cm,font=\sffamily\Large\bfseries},
  missing/.style={circle,font=\sffamily\Large,node distance=0.95cm},
  label/.style={node distance=1.2cm,rectangle,fill=white!50,draw=none,minimum size=0.7cm,font=\sffamily\normalsize},
  layer/.style={rectangle,fill=white!50,draw,minimum width=1.5cm,font=\sffamily\Large},
  loopStyle/.style={in=120,out=60, distance=2.5cm},
  weight/.style = {above,sloped,pos=0.3},]
\begin{figure}[!h]
 \centering
\begin{tikzpicture}[nn_style]

  \node[neuron]    (x0)       {};
  \node[neuron]    (x1)[below of=x0]   {};
  \node[neuron]    (x2)[below of=x1]   {};
  \node[missing]   (x3)[below of=x2]   {$\vdots$};
  \node[neuron]    (xn)[below of=x3]   {};
  
  
  \node[label]    (u0)[left of=x0]   {$\phi_0$};
  \node[label]    (u1)[left of=x1]   {$\phi_1$};
  \node[label]    (u2)[left of=x2]   {$\phi_2$};
  \node[label]    (un)[left of=xn]   {$\phi_n$};
  
  
  \node[layer] (hl)[ right of=x2,node distance=4cm] {$\Sigma$};
  \node[layer] (ol)[right of=hl,node distance=2.5cm] {$\sigma$};
  
  \node[label]  (phi)[right of=ol,node distance=2cm] {$\phi_j$};
   
  
  \path[->] (x0) edge [] node[weight]{$w_{0j}$}   (hl)
	    (x1) edge [] node[weight]{$w_{1j}$}   (hl)
	    (x2) edge [] node[weight]{$w_{2j}$}   (hl)
	    (xn) edge [] node[weight]{$w_{nj}$}   (hl)
	    (u0) edge []   (x0)
	    (u1) edge []   (x1)
	    (u2) edge []   (x2)
	    (un) edge []   (xn)
	    (hl) edge []   node[]{$a_{j}$}(ol)
	    (ol) edge []   (phi);

\end{tikzpicture}
\caption{Neuron model}
\label{neuron_model}
\end{figure}



So we can compute partial derivatives with respect to a single weight $w_{lj}$, using simply the chain rule, as 

$$\frac{\partial L}{\partial w_{lj}}=\frac{\partial L}{\partial a_l} \cdot \frac{\partial a_l}{\partial w_{lj}}=\delta_l \cdot \phi_j$$
where we put

\begin{equation}
\delta_l \triangleq \frac{\partial L}{\partial a_l}
\end{equation}



So we can easily compute $\delta_u = \frac{\partial L^{(i)}}{\partial a_u} $ for each output unit $u$ once we choose a differentiable loss function; note
that we don't need the weights for such a computation. 

Let $P(l)$ be the set of parents of neuron $l$, formally:
\begin{equation} 
P(l) = \{ k: \exists \text{ a link between $l$ and $k$ with weight } w_{lk} \}
\end{equation}

Again, simply using the chain rule, we can write, for each non output unit $l$:

\begin{equation}
\label{loss_deriv}
\delta_l = \sum_{k\in P(l)} \frac{\partial L^{(i)}}{\partial a_k} \cdot \frac{\partial a_k}{\partial a_l}= \sum_{k\in P(l)} \delta_k \cdot 
\frac{\partial a_k}{\partial \phi_l} \cdot \frac{\partial \phi_l}{\partial a_l} = \sum_{k\in P(l)} \delta_k \cdot 
w_{kl} \cdot \sigma'(a_l)
\end{equation}


For output units instead we can compute $\delta_u = \frac{\partial L^{(i)}}{\partial a_u} $ directly once we define the loss function.

\subsection{Backpropagation matrix notation}

Here we rewrite the previously derived equations in matrix notation.

Let us define the weight matrix $W_i \in \mathbb{R}_{(p(i),p(i-1))}$, whose element $W_{i,j}$ is the weight of the arc which links neuron j from level i-1 to neuron i from level i, where
$p(i)$ is the neuron number for $i^{th}$ level.

\begin{equation}
 \overrightarrow{\phi}_1 \triangleq \overrightarrow{x}
\end{equation}

\begin{equation}
 \overrightarrow{a}_{i+1} \triangleq W_{i+1} \cdot \overrightarrow{\phi}_i
\end{equation}

\begin{equation}
 \overrightarrow{\phi}_{i+1} \triangleq \sigma(\overrightarrow{a}_{i+1})
\end{equation}

where $\sigma(\cdot)$ is the non-linear activation function and it's applied element by element.
We can rewrite equation \ref{loss_deriv} in matrix notation as:

\begin{equation}
 \frac{\partial L}{\partial W_i} = \frac{\partial L}{\partial \overrightarrow{a}_{i}} \cdot\frac{\partial \overrightarrow{a}_{i}}{\partial W_i}^T =
 \Delta_i \cdot \overrightarrow{\phi}_{i-1}^T
\end{equation}

where
\begin{equation}
\Delta_i  \triangleq  \frac{\partial L}{\partial \overrightarrow{a}_{i}} 
\end{equation}

\begin{equation}
 \Delta_i = W_{i+1}^T \cdot \Delta_{i+1} \circ \sigma(\Delta_i)
\end{equation}




\newpage
% \nocite{*}		 % Mostra in bibliografia anche gli oggetti non citati 
\bibliography{biblio}{}
\bibliographystyle{plain}


\end{document}     