\select@language {UKenglish}
\contentsline {chapter}{\numberline {1}Artificial neural networks}{5}{chapter.1}
\contentsline {section}{\numberline {1.1}A family of models}{5}{section.1.1}
\contentsline {section}{\numberline {1.2}Feed foward neural networks}{7}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}On expressivness of ffnn}{8}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Learning with ffnn}{8}{subsection.1.2.2}
\contentsline {subsection}{\numberline {1.2.3}Gradient}{9}{subsection.1.2.3}
\contentsline {paragraph}{Backpropagation}{11}{section*.2}
\contentsline {section}{\numberline {1.3}Recurrent neural networks}{11}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}On expressivness of rnn}{13}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Learning with ffnn}{14}{subsection.1.3.2}
\contentsline {subsection}{\numberline {1.3.3}Gradient}{14}{subsection.1.3.3}
\contentsline {paragraph}{Backpropagation through time (BPTT)}{16}{section*.3}
\contentsline {subsection}{\numberline {1.3.4}The vanishing and exploding gradient problem}{16}{subsection.1.3.4}
\contentsline {paragraph}{Hochreiter Analysis: Weak upper bound}{17}{section*.4}
\contentsline {paragraph}{Explaining the problem using network's graph}{18}{section*.5}
\contentsline {paragraph}{The ReLU case}{18}{section*.6}
\contentsline {section}{\numberline {1.4}Activation functions and gradient}{20}{section.1.4}
\contentsline {paragraph}{Sigmoid}{20}{section*.7}
\contentsline {paragraph}{ReLU}{20}{section*.8}
\contentsline {chapter}{\numberline {A}Notation}{23}{appendix.A}
\contentsline {paragraph}{Derivative with respect to vector}{23}{section*.9}
\contentsline {paragraph}{Derivative with respect to matrix}{23}{section*.10}
