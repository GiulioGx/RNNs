\select@language {UKenglish}
\contentsline {chapter}{\numberline {1}Artificial neural networks}{5}{chapter.1}
\contentsline {section}{\numberline {1.1}A family of models}{5}{section.1.1}
\contentsline {paragraph}{The activation function}{6}{section*.2}
\contentsline {paragraph}{The bias term}{7}{section*.3}
\contentsline {paragraph}{Layered view of the net}{7}{section*.4}
\contentsline {section}{\numberline {1.2}Feed foward neural networks}{8}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Learning with FFNNs}{9}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Gradient}{10}{subsection.1.2.2}
\contentsline {paragraph}{Backpropagation}{11}{section*.5}
\contentsline {section}{\numberline {1.3}Recurrent neural networks}{12}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Learning with RNNs}{14}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Gradient}{15}{subsection.1.3.2}
\contentsline {paragraph}{Backpropagation through time (BPTT)}{17}{section*.6}
\contentsline {subsection}{\numberline {1.3.3}The vanishing and exploding gradient problem}{17}{subsection.1.3.3}
\contentsline {paragraph}{Hochreiter Analysis: Weak upper bound}{18}{section*.7}
\contentsline {paragraph}{Explaining the problem using network's graph}{19}{section*.8}
\contentsline {paragraph}{The ReLU case}{20}{section*.9}
\contentsline {section}{\numberline {1.4}Activation functions and gradient}{21}{section.1.4}
\contentsline {paragraph}{Sigmoid}{21}{section*.10}
\contentsline {paragraph}{Tanh}{21}{section*.11}
\contentsline {paragraph}{ReLU}{21}{section*.12}
\contentsline {section}{\numberline {1.5}On expressiveness}{24}{section.1.5}
\contentsline {paragraph}{Understanding the difference between RNN and FFNN models}{25}{section*.13}
\contentsline {chapter}{\numberline {A}Notation}{27}{appendix.A}
\contentsline {paragraph}{Derivative with respect to vector}{27}{section*.14}
\contentsline {paragraph}{Derivative with respect to matrix}{27}{section*.15}
