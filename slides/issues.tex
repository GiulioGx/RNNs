 
\begin{frame}{Open Issues: Initialization}
	
	\begin{itemize}
		\item Some tasks, like the XOR one, are still "unresolved" (even for the other approaches). They cannot be solved with most of the seeds used for the initialization
		\item it seems to be an \textbf{initialization} matter
	\end{itemize}
	
	Popular strategies for initialization are:
	\begin{itemize}
		\item "small random weights", usually drawn from gaussian or uniform distribution with zero mean (Pascanu). 
		\item sparse initialization: only some weights are actually sampled from a distribution, the other are set to zero (HF).
		\item ESN-like initialization\cite{reservoirSummary}.
	\end{itemize}
	
\end{frame}

\begin{frame}{Open Issues: Learning rate}
	
	\begin{itemize}	
		\item 	the \textbf{learning rate} is usually tuned by hand, there is no convergence theory for SGD in the non convex case
		\item a \textbf{gradient clipping} technique is often employed:
		\begin{algorithm}[H]
			$\vec{g} \gets \nabla_{\vec{x}} L$\\
			\If{$\norm{\vec{g}} \geq threshold $}{$\vec{g} \gets \frac{threshold}{\norm{\vec{g}}} \vec{g}$}
			\caption{Gradient clipping}
			\label{algo:gradClipping}
		\end{algorithm}
		
		\item  some \textbf{momentum} or \textbf{averaging} technique often yield better convergence time, again tuned by hand
	\end{itemize}
	

\end{frame}