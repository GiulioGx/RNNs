 
\begin{frame}{The model}
	
\begin{block}{RNN}
	Given an input sequences $\{\vec{u}\}_{t=1,...,T}$, with $ \vec{u}_t \in \mathbb{R}^p$, the output sequence of a RNN $\{\vec{y}\}_{t=1,...,T}$, with $\vec{y}_t \in \mathbb{R}^o$,  is defined by the following:
\begin{align}
		&\vec{y}^t \defeq F(W^{out}\cdot\vec{a}^t + \vec{b}^{out})\\
		&\vec{a}^t \defeq W^{rec}\cdot\vec{h}^{t-1}+W^{in}\cdot\vec{u}^t+\vec{b}^{rec}\\
		&\vec{h}^t \defeq  \sigma(\vec{a}^t) \\
		&\vec{h}^0 \defeq \overrightarrow{0},
\end{align}
where $\sigma(\cdot):\mathbb{R}\rightarrow\mathbb{R}$ is a non linear function applied element-wise called activation function.
\end{block}
\end{frame}

\begin{frame}
	\tikzstyle{rnn_style}=[->,shorten >=1pt,auto,node distance=1.5cm,
	thick,
	neuron/.style={circle,fill=white!50,node distance =1.2cm,draw,minimum size=0.7cm,font=\sffamily\Large\bfseries},
	missing/.style={rectangle,node distance =1.2cm,fill=white!50,draw=none,minimum size=0.7cm,font=\sffamily\Huge\bfseries},
	label/.style={node distance=1.2cm,rectangle,fill=white!50,draw=none,minimum size=0.7cm,font=\sffamily\normalsize},
	layer/.style={rectangle,fill=white!50,draw,minimum width=4cm,font=\sffamily\normalsize},
	loopStyle/.style={in=120,out=60, distance=2.5cm},]
	\begin{figure}[!h]
		\centering
		\resizebox{7cm}{!}{
		\begin{tikzpicture}[rnn_style]
		
		\node[neuron]    (x0)       {};
		\node[neuron]    (x1)[right of=x0]   {};
		\node[neuron]    (x2)[right of=x1]   {};
		\node[missing]   (x3)[right of=x2]   { $\hdots$};
		\node[neuron]    (xn)[right of=x3]   {};
		
		\node[label]    (u0)[below of=x0]   {$u_0[t]$};
		\node[label]    (u1)[below of=x1]   {$u_1[t]$};
		\node[label]    (u2)[below of=x2]   {$u_2[t]$};
		\node[label]    (un)[below of=xn]   {$u_n[t]$};
		
		
		\node[layer] (hl)[above of=x2,node distance=2cm] {Hidden layer};
		\node[neuron](b) [right of=hl,node distance=3cm] {};
		\node[label] (b_l) [right of=b] {bias=1};
		\node[layer] (ol)[above of=hl,node distance=2cm] {Output layer};
		
		\node[neuron] (o1) at (0,5.5) {};
		\node[neuron] (o2)[right of=o1] {};
		\node[neuron] (o3)[right of=o2] {};
		\node[missing](o4)[right of=o3] {$\hdots$};
		\node[neuron] (on)[right of=o4] {};
		
		
		\node[label]    (y0)[above of=o1]   {$y_0[t]$};
		\node[label]    (y1)[above of=o2]   {$y_1[t]$};
		\node[label]    (y2)[above of=o3]   {$y_2[t]$};
		\node[label]    (yn)[above of=on]   {$y_n[t]$};
		
		
		\path[->] (x0) edge [] node[]{$W_{in}$}   (hl)
		(x1) edge []   (hl)
		(x2) edge []   (hl)
		(xn) edge []   (hl)
		(u0) edge []   (x0)
		(u1) edge []   (x1)
		(u2) edge []   (x2)
		(un) edge []   (xn)
		(ol) edge []   (o1)
		(ol) edge []   (o2)
		(ol) edge []   (o3)
		(ol) edge []   (on)
		(o1) edge []   (y0)
		(o2) edge []   (y1)
		(o3) edge []   (y2)
		(on) edge []   (yn)
		(hl) edge []  node[]{$W_{out}$} (ol)
		(b)  edge [bend left,dotted,in= 160]  node[]{$b_{rec}$} (hl)
		(b)  edge [bend left,dotted,anchor=west, in= -160]  node[]{$b_{out}$} (ol)
		(hl) edge [loop ,in=-160,out=160, distance=3cm,anchor=east ]      node [align=center]  {$W_{rec} $} (hl);
		
		\end{tikzpicture}
		}
		\caption{$\net{RNN}$ model.}
		\label{rnn_model}
	\end{figure}
\end{frame}

\begin{frame}{The optimization problem}
Given a dataset $D$:
\begin{equation}
D\defeq\{\{\overline{\vec{u}}^{(i)}\}_{t=1,...,T}, \overline{\vec{u}}^{(i)}_t \in \mathbb{R}^p, \{\overline{\vec{y}}^{(i)}\}_{t=1,...,T}, \overline{\vec{y}}^{(i)}_t \in \mathbb{R}^o;  i=1,...,N\}
\end{equation}
we define a loss function $L_D:\mathbb{R}^N \rightarrow \mathbb{R}_{\geq 0}$ over $D$  as
\begin{equation}
L_D(\vec{x})\defeq\frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T L_t(\overline{\vec{y}}_t^{(i)},\vec{y}_t^{(i)}(\vec{x})),
\end{equation}
where $L_t(\cdot, \cdot)$ is an arbitrary loss function for the time step $t$.
The problem is \begin{equation}
\min_{\vec{x}\in \mathbb{R}^N} L_D(\vec{x})
\end{equation}
\end{frame}

\begin{frame}{Stochastic gradient descent (SGD)}
\begin{algorithm}[H]
	\KwData{\\
		\Indp
		$D=\{\pair{\vec{u}^{(i)}}{\vec{y}^{(i)}}\}$: training set\\
		$\vec{x}_0$: candidate solution \\
		$m$: size of each minibatch\\
	}
	
	\KwResult{\\
		\Indp $\vec{x}$: solution
	}
	\BlankLine
	
	$\vec{x} \gets \vec{x}_0$\\
	\While{stop criterion}{
		
		$I$ $\gets$ select $m$ training example $\in D$  \\
		$\alpha \gets$ compute learning rate \\
		$\vec{x} \gets \vec{x} - \alpha \sum_{ i\in I}\nabla_{\vec{x}} L(\vec{x}; \pair{\vec{u}^{(i)}}{\vec{y}^{(i)}})$\\
	}
	\caption{Stochastic gradient descent}
	\label{algo:sgd}
\end{algorithm}	
\end{frame}

\begin{frame}{Stochastic gradient descent (SGD)}
	\begin{itemize}
		\item Nemirovski (2009)\cite{Nemirovski}: proof of convergence in the convex case
		\item there are no theoretical guarantees in the non-convex case 
		\item in practice it always works: SGD is the standard framework in most of neural networks applications.
	\end{itemize}
\end{frame}