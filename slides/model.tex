
\subsection{The model}
\begin{frame}{The model}
	
%\begin{block}{Definition of RNN}
\textbf{Definition of RNN}
\\
	Given an input sequence $\{\vec{u}^t\}_{t=1,...,T}$, with $ \vec{u}^t \in \mathbb{R}^p$, an RNN yields the output sequence $\{\vec{y}^t\}_{t=1,...,T}$, with $\vec{y}^t \in \mathbb{R}^o$,  defined by the following:
\begin{align}
		&\vec{y}^t \defeq F(\vec{z}^t)\\
		&\vec{z}^t \defeq \mat{W}^{out}\cdot\vec{a}^t + \vec{b}^{out}\\
		&\vec{a}^t \defeq \mat{W}^{rec}\cdot\vec{h}^{t-1}+\mat{W}^{in}\cdot\vec{u}^t+\vec{b}^{rec}\\
		&\vec{h}^t \defeq  \sigma(\vec{a}^t) \\
		&\vec{h}^0 \defeq \vec{0},
\end{align}
where $\sigma(\cdot):\mathbb{R}\rightarrow\mathbb{R}$ is a non linear function applied element-wise called \textbf{activation function}, $F(\cdot)$ is called \textbf{output function}. 
%\end{block}

%\begin{block}{}
	The \textbf{parameters} of the net are $\{\mat{W}^{out}, \mat{W}^{in},  \mat{W}^{rec}, \vec{b}^{rec}, \vec{b}^{out}\}$.
%\end{block}
\end{frame}

\begin{frame}{The model}
	\begin{figure}[!h]
		\centering
		\resizebox{7cm}{!}{
		\begin{tikzpicture}[RNN_style, loopStyle/.style={in=120,out=60, distance=2.5cm}]
		
		\node[neuron]    (x0)       {};
		\node[neuron]    (x1)[right of=x0]   {};
		\node[neuron]    (x2)[right of=x1]   {};
		\node[missing]   (x3)[right of=x2]   { $\hdots$};
		\node[neuron]    (xn)[right of=x3]   {};
		
		\node[label]    (u0)[below of=x0]   {$u_0^t$};
		\node[label]    (u1)[below of=x1]   {$u_1^t$};
		\node[label]    (u2)[below of=x2]   {$u_2^t$};
		\node[label]    (un)[below of=xn]   {$u_n^t$};
		
		
		\node[layer] (hl)[above of=x2,node distance=2cm] {Hidden layer};
		\node[neuron](b) [right of=hl,node distance=3cm] {};
		\node[label] (b_l) [right of=b] {bias=1};
		\node[layer] (ol)[above of=hl,node distance=2cm] {Output layer};
		
		\node[neuron] (o1) at (0,5.5) {};
		\node[neuron] (o2)[right of=o1] {};
		\node[neuron] (o3)[right of=o2] {};
		\node[missing](o4)[right of=o3] {$\hdots$};
		\node[neuron] (on)[right of=o4] {};
		
		
		\node[label]    (y0)[above of=o1]   {$y_0^t$};
		\node[label]    (y1)[above of=o2]   {$y_1^t$};
		\node[label]    (y2)[above of=o3]   {$y_2^t$};
		\node[label]    (yn)[above of=on]   {$y_n^t$};
		
		
		\path[->] (x0) edge [] node[]{$W_{in}$}   (hl)
		(x1) edge []   (hl)
		(x2) edge []   (hl)
		(xn) edge []   (hl)
		(u0) edge []   (x0)
		(u1) edge []   (x1)
		(u2) edge []   (x2)
		(un) edge []   (xn)
		(ol) edge []   (o1)
		(ol) edge []   (o2)
		(ol) edge []   (o3)
		(ol) edge []   (on)
		(o1) edge []   (y0)
		(o2) edge []   (y1)
		(o3) edge []   (y2)
		(on) edge []   (yn)
		(hl) edge []  node[]{$W_{out}$} (ol)
		(b)  edge [bend left,dotted,in= 160]  node[]{$b_{rec}$} (hl)
		(b)  edge [bend left,dotted,anchor=west, in= -160]  node[]{$b_{out}$} (ol)
		(hl) edge [loop ,in=-160,out=160, distance=3cm,anchor=east ]      node [align=center]  {$W_{rec} $} (hl);
		
		\end{tikzpicture}
		}
%		\caption{$\net{RNN}$ model.}
		\label{rnn_model}
	\end{figure}
\end{frame}

\subsection{The optimization problem}
\begin{frame}{The optimization problem}
Given a dataset $D$:
%\begin{equation}
%D\defeq\{\{\overline{\vec{u}}^t\}_{t=1,...,T}^{(i)}, \overline{\vec{u}}^{(i)}_t \in \mathbb{R}^p, \{\overline{\vec{y}}^{(i)}\}_{t=1,...,T}, \overline{\vec{y}}^{(i)}_t \in \mathbb{R}^o;  i=1,...,N\}
%\end{equation}
\begin{equation}
	D\defeq\{\{\overline{\vec{u}}^t\}_{t=1,...,T(i)}^{(i)}, \{\overline{\vec{y}}^t\}_{t=1,...,T(i)}^{(i)};  i=1,...,N\}
\end{equation}
we define a loss function $L_D(\vec{x})$ over $D$  as
\begin{equation}
L_D(\vec{x})\defeq\frac{1}{N}\sum_{i=1}^N \sum_{t=1}^{T(i)} L_t(\overline{\vec{y}}_t^{(i)},\vec{y}_t^{(i)}(\vec{x})),
\end{equation}
where $L_t(\cdot, \cdot)$ is an arbitrary loss function for the time step $t$ and $\vec{x}$ represents all the parameters of the network.
The problem is \begin{equation}
\min_{\vec{x}} L_D(\vec{x}).
\end{equation}
\end{frame}

\begin{frame}{Some learning examples}

	\begin{itemize}
		\item Regression: mean squared error, linear output
		\begin{equation}
			L(\vec{y}, \vec{t}) = \frac{1}{M}\sum_{i=1}^M (y_i-t_i)^2, \quad F(\vec{y}) = \vec{y}.
		\end{equation}
		\item Binary classification: hinge loss, linear output
		\begin{equation}
			L(y, t) = \max(0,1-t\cdot y ), \quad F(y) = y.
		\end{equation}
		\item Multi-way classification: cross entropy loss, softmax output
		\begin{equation}
						L(\vec{y}, \vec{t}) = -\frac{1}{M}\sum_{i=1}^M \log(y_i)\cdot t_i, \quad F(y_j) = \frac{e^{y_j}}{\sum_{i=1}^M e^{y_i}}.
		\end{equation}
	\end{itemize}
\end{frame}

\begin{frame}{Stochastic gradient descent (SGD)}
\begin{algorithm}[H]
	\KwData{\\
		\Indp
		$D=\{\pair{\vec{u}^{(i)}}{\vec{y}^{(i)}}\}$: training set\\
		$\vec{x}_0$: candidate solution \\
		$m$: size of each mini-batch\\
	}
	
	\KwResult{\\
		\Indp $\vec{x}$: solution
	}
	\BlankLine
	
	$\vec{x} \gets \vec{x}_0$\\
	\While{stop criterion}{
		
		$I$ $\gets$ select $m$ training example $\in D$  \\
		$\alpha \gets$ compute learning rate \\
		$\vec{x} \gets \vec{x} - \alpha \sum_{ i\in I}\nabla_{\vec{x}} L(\vec{x}; \pair{\vec{u}^{(i)}}{\vec{y}^{(i)}})$\\
	}
	\caption{Stochastic gradient descent}
	\label{algo:sgd}
\end{algorithm}	
\end{frame}
%\begin{frame}{Convergence of SDG}
%	\begin{itemize}
%		\item Nemirovski (2009)\cite{Nemirovski}: proof of convergence in the convex case
%		\item there are no theoretical guarantees in the non-convex case 
%		\item in practice it always works: SGD is the standard framework in most of neural networks applications.
%	\end{itemize}
%\end{frame}

